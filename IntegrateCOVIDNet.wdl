# This file is generated by create_usherPlaceNewSamples_fullRefreshRun_w_Cluster_script.sh
version 1.0

workflow IntegrateCOVIDNet {
    input {
        String prefix= "CDPH"
        String terra_project
        String workspace_name
        Array[String] table_name
        # File previous_names
        File public_meta
        File protobuf
        String public_json_bucket
        Int treesize = 500
        File state_and_county_lex
        File us_states_w_ca_counties_geo
        File us_states_geo
        File script
        File permitted_specimens
        File genbank
        File overlay
        # File gisaid_meta
        File airport_p
        File airport_c
    }
    parameter_meta {
        table_name : "List of Terra table names containing SARS-CoV-2 sequences and metadata"
        terra_project : "Project in which to find the table names"
        workspace_name : "Workspace containing terra_project"
        protobuf: "UShER-generated phylogenetic tree in protobuf format. The program will add the new samples to this tree."
        # previous_names: "IDs saved from the last time this workflow was run."
        public_meta: "Metadata for the input protobuf tree containing sample names in first column"
        prefix: "A short string to prepend to the input sample IDs during the run. This is important if sample IDs are numeric: UShER will treat these as existing node IDs and ignore the sample."
        public_json_bucket: "Location for the output subtrees; files will be put in a subdirectory called jsontrees. This must be a public location so Nextstrain can pull in the trees for display. Note that the user's service account must have write access to this bucket."
        treesize: "Number of neighbors in the output json subtrees. If input samples end up in the same subtree, the matUtils output simply links both sample IDs to the same file."

    }
    call prefixSanityCheck { input: prefix=prefix }
    call parse_terratable { 
        input : 
            prefix = prefix,
            terra_project = terra_project,
            workspace_name = workspace_name,
            table_name = table_name,
            metatsv = public_meta,
            permitted_specimens = permitted_specimens
    }
    call gcs_copy_fafiles { 
        input : 
            cdph_falist = parse_terratable.cdphfastas, 
            cdc_falist = parse_terratable.cdcfastas, 
            lhj_falist = parse_terratable.lhjfastas, 
            vrdl_falist = parse_terratable.vrdlfastas, 
            prefix=prefix 
    }
    call getProblemVcf
    call mafft_align { 
        input : 
            sequences = gcs_copy_fafiles.combined_fasta, 
            ref_fasta = getProblemVcf.ref_fasta 
    }
    call faToVcf {
        input :
            fasta       = mafft_align.aligned_sequences,
            problem_vcf = getProblemVcf.problem_vcf
    }
    call usherAddSamples { 
        input : 
            vcf      = faToVcf.sample_vcf,
            protobuf = protobuf
    }
    call Extract {
        input : 
             tree_pb = usherAddSamples.new_tree,
             public_meta=public_meta,
             # samples_meta=parseGisaidMetadata.sample_meta_plus, 
             samples_meta=parse_terratable.sample_meta, 
             prefix=prefix,
             treesize=treesize,
             public_json_bucket=public_json_bucket,
             script = script
             # genbank = genbank
    }
    call usher_to_taxonium {
        input :
            protobuf = usherAddSamples.new_tree,
            public_meta = public_meta,
            genbank = genbank,
            overlay = overlay
    }
    call prepareCountyData {
        input :
            state_and_county_lex = state_and_county_lex,
            public_meta = public_meta,
            # samples = parseGisaidMetadata.sample_meta_plus,
            samples = parse_terratable.sample_meta,
            airport_p = airport_p,
            airport_c = airport_c
    }
    call introduce {
        input : 
            cleaned = usherAddSamples.new_tree,
            regions = prepareCountyData.regions,
            date_file = prepareCountyData.date_file
    }
    call introduce_states {
        input : 
            cleaned_us = usherAddSamples.new_tree,
            regions_us = prepareCountyData.regions_us,
            date_file_us = prepareCountyData.date_file_us
    }
    call updateJavascript {
        input :
            state_and_county_lex = state_and_county_lex,
            us_states_w_ca_counties_geo = us_states_w_ca_counties_geo,
            clusters_counties = introduce.clusters,
            us_states_geo = us_states_geo,
            clusters_states = introduce_states.clusters_us
    }
    call generateDisplayTables {
        input :
            clusters_counties = introduce.clusters,
            county_pids = prepareCountyData.pids 
    }
    call generateDisplayTables_states {
        input :
            clusters_states = introduce_states.clusters_us,
            state_pids = prepareCountyData.pids
    }
    call prepareTaxonium {
        input :
            clusters = introduce.clusters,
            regions = prepareCountyData.regions,
            merged = prepareCountyData.merged
    }
    call prepareTaxonium_states {
        input :
            clusters_us = introduce_states.clusters_us,
            regions_us = prepareCountyData.regions_us,
            merged_us = prepareCountyData.merged
    }  
    call prepareJSONL {
        input :
            protobuf = usherAddSamples.new_tree,
            cluster_swap = prepareTaxonium.cluster_swap,
            genbank = genbank,
            overlay = overlay
    }
    call prepareJSONL_states {
        input : 
            protobuf = usherAddSamples.new_tree,
            cluster_swap_us = prepareTaxonium_states.cluster_swap_us,
            genbank = genbank,
            overlay = overlay
    }
    call gcs_copy {
        input :
            infiles = Extract.subtree_jsons, 
            gcs_uri_prefix = Extract.bucket,
            html = Extract.out_html,
            paui2url = Extract.paui_mapped_to_tree,
            taxonium = Extract.taxodium_file,
            name = Extract.subtree_folder,
            subtree = Extract.out_json,
            js_regions = updateJavascript.js_regions,
            js_regions_us = updateJavascript.js_regions_us,
            cview_jsonl = prepareJSONL.cview_jsonl,
            cview_us_jsonl = prepareJSONL_states.cview_us_jsonl,
            cluster_data = generateDisplayTables.cluster_data,
            cluster_data_us = generateDisplayTables_states.cluster_data_us,
            sample_data = generateDisplayTables.sample_data,
            sample_data_us = generateDisplayTables_states.sample_data_us,
            pids = generateDisplayTables.pids,
            pids_us = generateDisplayTables_states.pids_us,
            download_clusters = introduce.clusters,
            download_clusters_us = introduce_states.clusters_us,
            public_meta = public_meta,
            new_tree = usherAddSamples.new_tree,
            translation_table = Extract.translation_table,
            sample_meta = parse_terratable.sample_meta,
            # sample_meta = parseGisaidMetadata.sample_meta_plus,
            updating = prepareJSONL.updating,
            okay = prepareJSONL.okay,
            tree_jsonl = usher_to_taxonium.tree_jsonl
    }
    output {
        File noPassTable= parse_terratable.noPassIds
        File permitted_list = parse_terratable.permitted_list
        # File sample_meta = parseGisaidMetadata.sample_meta_plus
        File sample_meta = parse_terratable.sample_meta
        # File new_gisaid_rows = parseGisaidMetadata.test
        File mapping_table = parse_terratable.mapping_table
        File newProtobuf = usherAddSamples.new_tree
        File urltable = Extract.out_html
        File subtree_assignments = Extract.out_tsv
        File name_and_paui = Extract.specimen_mapped_to_paui
        File paui2url = Extract.paui_mapped_to_tree
        String subtree_link = "http://storage.googleapis.com/" + Extract.bucket + "/samples_subtrees.html"
        File taxodium_pb = Extract.taxodium_file
        File matUtils_translate = Extract.translation_table
        File out_json = Extract.out_json 
        File subtree_folder = Extract.subtree_folder
        File merged = prepareCountyData.merged
        File regions = prepareCountyData.regions
        File pids = generateDisplayTables.pids 
        File date_file = prepareCountyData.date_file
        File regions_us = prepareCountyData.regions_us
        File pids_us = generateDisplayTables_states.pids_us
        File date_file_us = prepareCountyData.date_file_us
        File js_regions = updateJavascript.js_regions
        File download_clusters = introduce.clusters
        File cluster_data = generateDisplayTables.cluster_data
        File cluster_data_us = generateDisplayTables_states.cluster_data_us
        File sample_data = generateDisplayTables.sample_data
        File sample_data_us = generateDisplayTables_states.sample_data_us
        File cluster_swap = prepareTaxonium.cluster_swap
        File cview_jsonl = prepareJSONL.cview_jsonl 
        File cview_us_jsonl = prepareJSONL_states.cview_us_jsonl
        File fasta_sources = parse_terratable.fasta_sources
        File js_regions_us = updateJavascript.js_regions_us
        File download_clusters_us = introduce_states.clusters_us
        File cluster_swap_us = prepareTaxonium_states.cluster_swap_us
        File cdph_fa_urls = parse_terratable.cdphfastas
        File cdc_fa_urls = parse_terratable.cdcfastas
        File lhj_fa_urls = parse_terratable.lhjfastas
        File vrdl_fa_urls = parse_terratable.vrdlfastas
        File no_permission = parse_terratable.no_permission
        File aggregate = parse_terratable.aggregate
        File tree_jsonl = usher_to_taxonium.tree_jsonl
    }
    meta {
        author: "Marc Perry"
        email: "madperry@ucsc.edu"
        description: "Runs the UShER phylogenetic SARS-CoV-2 tree builder on samples in input tables; outputs json format subtrees for each sample"
    }
}

task prefixSanityCheck{
     meta { description: "No dashes, slashes or other nonsense in the input string" }
     input { String prefix }
     command <<<
         if [[ ! ~{prefix} =~ ^[A-Za-z0-9_]+$ ]]; then
             >&2 echo "prefix ~{prefix} contains unallowed character, only use A-Z, a-z, 0-9, _"
             exit 1
         fi
     >>>
    runtime {
        docker: "pathogengenomics/usher:latest"
    }
}

# parse_terratable_task.wdl is generated by create_usherPlaceNewSamples_fullRefreshRun_w_Cluster_script.sh
task parse_terratable{
    input{
        String prefix 
        String terra_project
        String workspace_name
        Array[String] table_name
        File metatsv
        File permitted_specimens

        Int api_retries = 5   # maximum number of firecloud API retries PER TABLE
        Int wait_seconds = 45 # seconds to wait before retrying a firecloud API call
    }
    meta {
        description: "Pull tables and select samples with decent sequences. Match Gisaid ID to those already in the UShER tree. Create metadata file"
        volatile: false
    }
    command <<<
    set -euo pipefail

    curl "https://hgwdev.gi.ucsc.edu/~madperry/vbl_combined_specimen.tsv" > vbl_combined_specimen.tsv

    # The daily updated file named public.plusGisaid.latest.metadata.tsv preferentially uses non-GISAID accession IDs (where possible)
    # This is a 4-column TSV that maps accession_ids: c1, GISAID accession, c2, GenBank accession, c3, strain, c4, collection_date
    curl "https://hgwdev.gi.ucsc.edu/~angie/epiToPublicAndDate.latest" > epiToPublicAndDate.latest

    python3 <<CODE
try:
    api_retries = ~{api_retries}
    wait_seconds = ~{wait_seconds}
except NameError:
    api_retries = 5
    wait_seconds = 45
workspace_project = '~{terra_project}'
workspace_name = '~{workspace_name}'
tnames = '~{sep=',' table_name}'
metatsv = '~{metatsv}'
prefix = '~{prefix}'
permitted_specimens = '~{permitted_specimens}'
DEFAULT_PAGE_SIZE = 1000

import sys    
import csv
import json
import collections
import re
import subprocess
import copy
import time
import logging
from firecloud import api as fapi
from tqdm import tqdm 
import argparse
import math

logging.basicConfig(level=logging.DEBUG)
logging.getLogger('chardet').setLevel(logging.CRITICAL+1)

def print_debugging_information(raw, raw_as_text, table=None):
    logging.warning("-- Dumping information for debugging purposes --")
    logging.warning(f"FireCloud raw response, as text (type {type(raw_as_text)}): {raw_as_text}") # do this first in case raw causes TypeError
    logging.warning(f"FireCloud raw response (type {type(raw)}): {raw}")
    if table is not None:
        logging.warning(f"Attempted table: (type {type(table)}) {table}")
    return

def the_worst_table(table_name):
    if table_name == 'vbl_combined_specimen':
        logging.info("Attempting to take vlb_combined_specimen from elsewhere...")
        table = []
        with open('vbl_combined_specimen.tsv', newline='') as csvFile:
          data = csv.DictReader(csvFile, delimiter = '\t')
          for row in data:
            name = row['vbl_combined_specimen_id']
            object = { 'attributes': row, 'name': name }
            table.append(object)
        return table
    else:
        logging.error("This is {table_name}. We don't have a fallback for it.")
        exit(1)

def yoink_table(workspace_project, workspace_name, table_name, retries=0):
    '''The FireCloud API is buggy and sometimes fumbles. We will keep poking it with
    a stick until it works. This function is recursive, so it does not make use of
    any timers.'''
    if retries > api_retries:
        logging.error("We called the FireCloud API five times and it's still being rude. Time to give up.")
        exit(1)
    try:
        raw = fapi.get_entities(workspace_project, workspace_name, table_name)
        raw_as_text = raw.text
        try:
            table = json.loads(raw_as_text)
        except JSONDecodeError:
            logging.warning("Caught a JSONDecodeError.")
            logging.warning("Raw as text: {raw_as_text}") # do this first as raw-raw might throw typeerror
            logging.warning("Rawest data: {raw}")
            logging.warning("Attempting a different workaround...")
            return the_worst_table(table_name)
        if table is None:
            logging.warning("FireCloud API returned a NoneType, but is pretending that's a success. This should never happen.")
            logging.warning(f"FireCloud raw response (type {type(raw)}): {raw}")
            logging.warning(f"FireCloud raw response, as text (type {type(raw_as_text)}): {raw_as_text}")
            logging.warning(f"Attempted table: (type {type(table)}) {table}")
            logging.warning("Retrying...")
            time.sleep(wait_seconds)
            new_retries = retries+1
            return yoink_table(workspace_project, workspace_name, table_name, retries=new_retries)
        else:
            try:
                logging.info(f"Called {table_name}, FireCloud API returned {type(table)} of length {len(table)}.")
                return table
            except TypeError:
                logging.warning("Caught TypeError while printing stats of FireCloud API. This should never happen.")
                logging.warning(f"FireCloud raw response (type {type(raw)}): {raw}")
                logging.warning(f"FireCloud raw response, as text (type {type(raw_as_text)}): {raw_as_text}")
                logging.warning(f"Attempted table: (type {type(table)}) {table}")
                logging.warning("Retrying...")
                time.sleep(wait_seconds)
                new_retries = retries+1
                return yoink_table(workspace_project, workspace_name, table_name, retries=new_retries)
    except OSError:
        logging.warning(f"FireCloud API threw an OSError on {table_name}. This is a known occasional random problem with the API. Retrying...")
        time.sleep(wait_seconds)
        new_retries = retries+1
        return yoink_table(workspace_project, workspace_name, table_name, retries=new_retries)

def get_entity_by_page(workspace_project, workspace_name, table_name, page, page_size=DEFAULT_PAGE_SIZE, sort_direction='asc', filter_terms=None):
    """Get entities from workspace by page given a page_size(number of entities/rows in entity table)."""
    # API = https://api.firecloud.org/#!/Entities/entityQuery
    response = fapi.get_entities_query(workspace_project, workspace_name, table_name, page=page,
                                       page_size=page_size, sort_direction=sort_direction,
                                       filter_terms=filter_terms)

    if response.status_code != 200:
        print(response.text)
        exit(1)

    return(response.json())


def use_alternative_api_call_for_large_terra_data_tables(workspace_project, workspace_name, entity_type, page_size=DEFAULT_PAGE_SIZE, attr_list=None):
    """Download large JSON object from Terra workspace by designated number of rows.
    Right now, we only use this for the CDC table, because it is monsterously large
    and breaks the yoink_table() function.
    Just like yoink_table(), this is a recursive function that will call itself iff
    the FireCloud API shits itself and throws an OSError. (But only during list_entity_types(),
    not the get_entity_by_page() call. Maybe that should get a try/except too.)
    However, yoink_table() uses fapi.get_entities(), while this function instead:
    1. Calls fapi.list_entity_types() to get everything in the workspace
    2. Figures out how many of those entities are related to the table we care about
       (which we passed in as entity_type)
    3. Do some magic to figure out how many ''pages'' we need to call (num_pages)
    4. Loop a call to the paginated API call get_entity_by_page() until we reach num_pages 

    API:
    * https://api.firecloud.org/#!/Entities/getEntityTypes
    """
    try:
        response = fapi.list_entity_types(workspace_project, workspace_name)
        if response.status_code != 200:
            print(response.text)
            exit(1)
    except OSError:
        logging.warning(f"FireCloud API threw an OSError when listing entity types. This is a known occasional random problem with the API. Retrying...")
        time.sleep(wait_seconds)
        return use_alternative_api_call_for_large_terra_data_tables(workspace_project, workspace_name, entity_type, page_size=DEFAULT_PAGE_SIZE, attr_list=None)


    ## What the fuck do I care about this for?
    # get/report # of entities + associated attributes(column names) of input entity type
    entity_types_json = response.json()
    entity_count = entity_types_json[entity_type]["count"]
    entity_id = entity_types_json[entity_type]["idName"]
    ## Oh right, this is the cool bit where you could theoretically request a certain subset of columns, if you were so inclined
    # if user provided list of specific attributes to return, else return all attributes
    if attr_list:
        all_attribute_names = entity_types_json[entity_type]["attributeNames"]
        attribute_names = [attr for attr in all_attribute_names if attr in attr_list]
    else:
        attribute_names = entity_types_json[entity_type]["attributeNames"]

    # add the entity_id value to list of attributes (not a default attribute of API response)
    attribute_names.insert(0, entity_id)

    ## When I finally got this code working in Terra (remember it was written to run as a freestanding Python script)
    ## Here is what I got here:
    ## "301236 cdc_specimen_03(s) to export."
    logging.info(f'{entity_count} {entity_type}(s) to export.')

    # set starting row value and calculate number of pages
    row_num = 0
    num_pages = int(math.ceil(float(entity_count) / page_size))


    # get entities by page where each page has page_size # of rows using API call
    ## Next line gets printed to stdout
    ## Getting all 302 pages of entity data.
    print(f'Getting all {num_pages} pages of entity data.')
    logging.info(f'Getting all {num_pages} pages of entity data.')
    all_page_responses = []
    for page in tqdm(range(1, num_pages + 1)):
        all_page_responses.append(get_entity_by_page(workspace_project, workspace_name, entity_type, page, page_size))

    # for each response(page) in all_page_responses[] - contains parameter metadata
    ## in stdout:
    ## Writing 301236 attributes to tsv file.
    # print(f'Writing {entity_count} attributes to tsv file.')
    print(f'Writing {entity_count} entities to Python object.')
    logging.info(f'Writing {entity_count} entities to Python object.')
    list_of_json_entities = []
    ## So let's talk this out, walk through this.  I have a list (array? dict?), well, it is iterable, anyway
    ## of 300 and something page responses, and each page must therefore have at least 1,000 entities (table rows)
    ## or records
    for page_response in tqdm(all_page_responses):
        ## Each page_response, must be an . . . object? (well, because in Python, everything is an object, right?)
        ## So, a complex dict then, and there is a key named "results", and the values of those results are . . . 
        ## Oooh, I am liking this, it looks like they are JSON versions of each row.  Now we're sucking diesel
        ## Do I have to do much else here? Maybe I can just collect all of these entity jsons in a new list, how about that?
        # for each set of attributes in results (no parameters) get attribute names and entity_id(name)
        for entity_json in page_response["results"]:
            attributes = entity_json["attributes"]
            name = entity_json["name"]
            # add name and value to dictionary of attributes
            attributes[entity_id] = name
            list_of_json_entities.append(entity_json)

            ## MDP: Seems to me like this next block of code is trying to build up a, list? dict? something, anyway
            ## For printing, but we are not trying to print anything
            ## I want to return a response, don't I?
            ## values = []
            # for each attribute(column name) in list of attribute names(all columns for entity)
            ## for attribute_name in attribute_names:
                ## value = ""
                # if entity's attribute(column) is in list of attributes from response, set response's attribute value
                ## if attribute_name in attributes:
                    ## value = attributes[attribute_name]

                ## values.append(str(value))

            ## NO:
            # tsvout.write("\t".join(values) + "\n")
            row_num += 1
    # After you process all of the page_responses, return this object, right?
    return(list_of_json_entities)


workspace_project = '~{terra_project}'
workspace_name = '~{workspace_name}'
tnames = '~{sep=',' table_name}'
metatsv = '~{metatsv}'
prefix = '~{prefix}'
permitted_specimens = '~{permitted_specimens}'

with open(permitted_specimens, 'r') as FH1:
    previously_tested = FH1.readlines()

for i,s in enumerate(previously_tested):
    previously_tested[i] = s.strip()

has_fasta_permission = set(previously_tested)

epiToGisaid = dict()
epiToSubmission = dict()
with open(metatsv, 'r') as m:
    m.readline()
    for line in m:
        strain = line.strip().split('\t')[0]
        if re.search(r"^USA/CA|^USA/SEARCH", strain):
            epiToGisaid[strain.split('|')[1]] = strain
            epiToSubmission[strain.split('/')[1]] = strain

mapping_table = dict()
with open("epiToPublicAndDate.latest", 'r') as accessions:
    accessions.readline()
    for line in accessions:
        gisaid = line.strip().split('\t')[0]
        genbank = line.strip().split('\t')[1]
        mapping_table[gisaid] = genbank

foi={
    'assembly_fasta':'',
    'percent_reference_coverage':'',
    'pango_lineage':'',
    'nextclade_clade':'',
    'gisaid_accession':'',
    'county':'',
    'collection_date':'',
    'paui':'',
    'sequencing_lab':'',
    'submission_id':'',
    'specimen_accession_number':'',
    'clearlabs_fasta':'',
    'aligned_bai':'',
    'aligned_bam':'',
    'assembly_length_unambiguous':'',
    'assembly_mean_coverage':'',
    'consensus_flagstat':'',
    'consensus_stats':'',
    'vadr_alerts_list':'',
    'vadr_num_alerts':''
}

noPassIds = []
cdphfastas = []
cdcfastas = []
lhjfastas = []
vrdlfastas = []
fasta_sources = []
no_permission = []
outrows = []
aggregate = [] 
seen = set()
submission_id_seen = set()
lhj_accession = set()
vrdl_accession = set()
mapped_usherIDs_or_strains = set()

logging.info("Beginning work on the first tables.")
for table_name in (tnames.split(',')):
  start = time.time()
  startyoink = time.time()
  table = yoink_table(workspace_project, workspace_name, table_name)
  endyoink = time.time()
  logging.info(f"Got {table_name} from the FireCloud API in {int(endyoink-startyoink)} seconds.")
  try:
    logging.debug(f"Now about to iterate {table_name}, which is a {type(table)} of length {len(table)}.")
    print(len(table)) # should catch the possible type error even if logging level skips above command
  except TypeError:
    logging.warning("The table is throwing a type error outside of the yoinker function. Ugh!")
    logging.warning(f"The rude table is {table_name}, which is a {type(table)}.")
    logging.warning(f"Here's its contents: {table}")
    logging.warning("We're going to try calling the yoinker again and hope for the best.")
    table = yoink_table(workspace_project, workspace_name, table_name)
  # type error is probably going to be caught by the logging print from above
  for row in table:
    addFlag = False
    hasCdphFasta = True
    hasSubmissionId = False
    hasBeenMapped = False
    rdict = dict([('usherID', prefix+row['name']), ('name',row['name'])] + [(k, r) for k, r in foi.items()] +
              [(k,r) for k, r in row['attributes'].items() if k in foi.keys()])
    rdict['specimen_id'] = rdict['name']
    rdict['terra_data_table_name'] = table_name
    adict = copy.deepcopy(rdict)
    aggregate.append(adict)
    # Does this COVIDNet sample from Terra have a fasta file?
    if rdict['assembly_fasta']=='':
        hasCdphFasta = False
    if re.search(r"EMPTY|mpty|fc-e5ba72e5-e0a2-462c-bba0-b7b4b25ce206", rdict['assembly_fasta']):
        noPassIds.append("{}\tfasta filename indicates EMPTY\n".format(rdict['name']))
        # also avoiding the seen[] set
        continue
    # Does this COVIDNet sample from Terra already have a gisaid_accession?
    # Keep in mind that as of 2022-03-30 there were 6666 specimens in the CDPH COVIDNet
    # tables in Terra that had a gisaid_accession but mysteriously lacked a fasta file
    elif re.search(r"EPI_ISL", rdict['gisaid_accession']):
        # Toggle this flag to True, we are going to add it as an entry in the
        # samplemeta.ts file
        addFlag = True
        # Does the gisaid_accession from Terra match one of dictionary keys we extracted
        # from the public Tree metadata file?
        try:
           rdict['usherID'] = epiToGisaid[rdict['gisaid_accession']]
           hasBeenMapped = True
           if rdict['submission_id'] in epiToSubmission:
               hasSubmissionId = True
           # } close if rdict conditional
        # } close try:
        except KeyError:
            # if it doesn't match initially, use the same gisaid accession and see if there
            # is a matching dictionary key in the duplicate sample mapping table
            # This value is usually a GenBank Accession
            # Now check if this corresponding GenBank accession matches one of the 
            # dictionary keys we extracted from the public Tree metadata file
            try:
                rdict['usherID'] = epiToGisaid[mapping_table[rdict['gisaid_accession']]]
                hasBeenMapped = True
                if rdict['submission_id'] in epiToSubmission:
                    hasSubmissionId = True
            except KeyError:
                # You might think that a sample with a gisaid_accession is good to go
                # and if we cannot find it in the tree/metadata then we need to add it to 
                # the tree, but nothing could be farther from the truth.  This is a sample that
                # Has very poor quality data so we don't want it in the tree
                # of sequences we want UShER to place in the public tree
                if hasCdphFasta:
                    noPassIds.append("{}\thas gisaid_accession but was not found in bigtree nightly build: QC issues most likely\n".format(rdict['name']))
                    continue
                else:
                    noPassIds.append("{}\tno fasta file, and couldn't map mysterious gisaid_accession\n".format(rdict['name']))
                    continue

    elif rdict['gisaid_accession'] == '' or rdict['gisaid_accession'] == 'unknown':
        # This specimen_id has no gisaid_accession in Terra
        # Check if it has a subimssion_id in the epiToSubmission dict
        if rdict['submission_id'] in epiToSubmission:
            hasSubmissionId = True
            addFlag = True
            # Now check to see if this submission_id was already used by another specimen (it happens)
            if rdict['submission_id'] in submission_id_seen:
                hasSubmissionId = False

        # NOW check for the QC Threshold before deciding how to proceed
        # Specifically, do not assume that having a possible submission_id is going to be a panacea
        # There is all kinds of crap in Terra, especially in the county lhj workspace
        if hasCdphFasta:
            # To reach here, the sample _HAS_ a fasta file, but does not have a gisaid_accession
            # Therefore we are going to test if it passes QC for coverage of the reference genome sequence
            if rdict['percent_reference_coverage'] == '':
                # failesQC
                noPassIds.append("{}\tapparently has a fasta file but the coverage field is empty or null\n".format(rdict['name'], rdict['percent_reference_coverage']))
                continue
            elif float(rdict['percent_reference_coverage']) < 83:
                # failsQC
                noPassIds.append("{}\tcoverage {} does not pass 83% threshold\n".format(rdict['name'], rdict['percent_reference_coverage']))
                continue
            else:
                # We want to eliminate specimens that would failQC, EVEN if they have a submission_id, okay? Right?
                if hasSubmissionId is True:
                    # If they pass all of these tests then they are in the public_meta tree and we DON'T need to 
                    # add this fasta file to the tree ourselves, so at this step skip ahead down to the bottom
                    rdict['usherID'] = epiToSubmission[rdict['submission_id']]
                    hasBeenMapped = True 
                # Passed QC filters but no gisaid_accession and no mappable submission_id then:
                # It passes tha final test and we want to add it to the samplemeta.tsv file, AND we want to send the 
                # sequence to UShER to be placed on the public tree
                # At some point in the future it will get a gisaid_accession or a genbank_accession
                # Check if we had permission to read this consensus.fasta URL previously
                elif hasCdphFasta:
                    # Implicit assumption: we do have permission to read all of the CdphFasta file URLs if we made it this far
                    # But check to be sure
                    if rdict['name'] not in has_fasta_permission:
                        # For the COVIDNet tables assembly_fasta is the field to check
                        # IF quiet-mode gsutil stat 'works' the return code is '0' but if it fails the return-code is '1'
                        permission = subprocess.run("gsutil -q stat " + rdict['assembly_fasta'], shell=True).returncode
                        if permission > 0:
                            # Skip this specimen if we do not have read permission for it's fasta file
                            no_permission.append(rdict['specimen_id'] + "\t" + rdict['sequencing_lab'] + "\t" + rdict['assembly_fasta'])
                            continue
                    # We should only get here one of two ways:
                    # we previously determined we have permission
                    # OR: we just determined that we have permission
                    # If we don't have permission DO NOT add this specimen to the samplemeta.tsv table AND
                    # DO NOT add the URL to its fasta file to the list of URLs to download
                    addFlag = True
                    cdphfastas.append(rdict['assembly_fasta'])
                    fasta_sources.append(rdict['sequencing_lab'] + "\t" + rdict['usherID'] + "\t" + rdict['assembly_fasta'])
        else:
            # No fasta file, AND no gisaid_accession, we are not counting these any longer, just a waste of time, we 
            # are not adding their specimen_ids to the seen set
            continue
    # To use taxoniumtools later on, we need to ensure that there are no duplicated strains/usherIDs in the first column
    # of the samplemeta.tsv table
    if hasBeenMapped is True:
        # If the current specimen was successfully mapped but that strain is already being used by a different specimen
        # (don't ask, it happens) Then we are skipping this specimen, as wonderful as it might be.
        if rdict['usherID'] in mapped_usherIDs_or_strains:
            continue
        else:
            # However, if this is the very first time that we have successfully mapped this specific strain or usherID
            # Then add this information to the list to keep track of it
            mapped_usherIDs_or_strains.add(rdict['usherID'])

    # Samples that pass this next test are printed out in samplemeta.tsv
    if addFlag is True and not row['name'] in seen:
        if hasSubmissionId:
            submission_id_seen.add(rdict['submission_id'])
        # } close if conditional

        [rdict.pop(key) for key in ['assembly_fasta', 'percent_reference_coverage', 'clearlabs_fasta', 'submission_id', 'terra_data_table_name', 'aligned_bai', 'aligned_bam', 'assembly_length_unambiguous', 'assembly_mean_coverage', 'consensus_flagstat', 'consensus_stats', 'vadr_alerts_list', 'vadr_num_alerts']]
        rdict = {k:(' ' if v=='' else v) for (k,v) in rdict.items()} # add whitespace in empty fields, don't ask me why
        outrows.append(rdict)
    seen.add(row['name'])
  end = time.time()
  now = subprocess.run("date", text=True, capture_output=True).stdout
  logging.info(f"Finished {table_name} in {int(end-start)} seconds at {now}.")

# Now open and parse the CDC-CA Table
# Changing the prefix string when processing these next samples
now = subprocess.run("date", text=True, capture_output=True).stdout
logging.info(f"Beginning work on the CDC tables at {now}.")
start = time.time()
prefix = "CA_CDC_"
cdc_table_name = "cdc_specimen_03"
cdc_workspace_name = "dataAnalysis_SARS-CoV-2_CA-CDC"

# Previous version that now fails:
# cdc_table = yoink_table(workspace_project, cdc_workspace_name, cdc_table_name)
# New version we are testing:
cdc_table = use_alternative_api_call_for_large_terra_data_tables(workspace_project, cdc_workspace_name, cdc_table_name, page_size=DEFAULT_PAGE_SIZE, attr_list=None)

for row in cdc_table:
    addFlag = False
    hasBeenMapped = False
    cdc_rdict = dict([('usherID', prefix+row['name']), ('name',row['name'])] + [(k, r) for k, r in foi.items()] +
              [(k,r) for k, r in row['attributes'].items() if k in foi.keys()])
    # Remove all the trailing elements from specimens that have specimen_ids from Aegis Science Corporation
    # To convert this: ASC210863691-B3039588
    # into this: ASC210863691
    m = re.search(r"(ASC\d+)-[A-Z]\d+", cdc_rdict['name'])
    if m:
        cdc_rdict['name'] = m.group(1)

    cdc_rdict['specimen_id'] = cdc_rdict['name']
    cdc_rdict['terra_data_table_name'] = cdc_table_name
    adict = copy.deepcopy(cdc_rdict)    
    aggregate.append(adict)     
    # First check if has a peculiar pattern in the name field
    if re.search(r"_L00", cdc_rdict['name']):
        continue
    # Second check if it has a gisaid_accession
    if re.search(r"EPI_ISL", cdc_rdict['gisaid_accession']):
        addFlag = True
        # test if it is in the epiToGisaid dict
        try:
           cdc_rdict['usherID'] = epiToGisaid[cdc_rdict['gisaid_accession']]
           hasBeenMapped = True
        except KeyError:
            try:
                cdc_rdict['usherID'] = epiToGisaid[mapping_table[cdc_rdict['gisaid_accession']]]
                hasBeenMapped = True
            except KeyError:
                if re.search(r"fasta", cdc_rdict['assembly_fasta']) and cdc_rdict['percent_reference_coverage']!='':
                    noPassIds.append("{}\thas gisaid_accession but was not found in bigtree nightly build: QC issues most likely\n".format(cdc_rdict['name']))
                    continue
                else:
                    addFlag = False
                    noPassIds.append("{}\tno fasta file, and couldn't map mysterious gisaid_accession\n".format(cdc_rdict['name']))
    elif cdc_rdict['gisaid_accession'] == '' or cdc_rdict['gisaid_accession'] == 'unknown':
        if re.search(r"fasta", cdc_rdict['assembly_fasta']):
            if cdc_rdict['percent_reference_coverage'] == '':
                noPassIds.append("{}\tapparently has a fasta file but the coverage field is empty or null\n".format(cdc_rdict['name'], cdc_rdict['percent_reference_coverage']))
                continue
            elif float(cdc_rdict['percent_reference_coverage']) < 70:
                noPassIds.append("{}\tcoverage {} does not pass 70% threshold\n".format(cdc_rdict['name'], cdc_rdict['percent_reference_coverage']))
                continue
            else:
                # New Block of test code: do we have read permission for this specimen's fasta file?
                if cdc_rdict['name'] not in has_fasta_permission:
                    permission = subprocess.run("gsutil -q stat " + cdc_rdict['assembly_fasta'], shell=True).returncode
                    if permission > 0:
                        no_permission.append(cdc_rdict['specimen_id'] + "\t" + cdc_rdict['sequencing_lab'] + "\t" + cdc_rdict['assembly_fasta'])
                        continue

                addFlag = True
                cdcfastas.append(cdc_rdict['assembly_fasta'])
                fasta_sources.append(cdc_rdict['sequencing_lab'] + "\t" + cdc_rdict['usherID'] + "\t" + cdc_rdict['assembly_fasta'])
        else:
            # As above, no gisaid_accession, no fasta file, we are no longer tracking or counting these,
            # And we are not adding them to the seen set
            continue

    if hasBeenMapped is True:
        # If the current specimen was successfully mapped but that strain is already being used by a different specimen
        # (don't ask, it happens) Then we are skipping this specimen, as wonderful as it might be.
        if cdc_rdict['usherID'] in mapped_usherIDs_or_strains:
            continue
        else:
            # However, if this is the very first time that we have successfully mapped this specific strain or usherID
            # Then add this information to the list to keep track of it
            mapped_usherIDs_or_strains.add(cdc_rdict['usherID'])

    if addFlag is True and not row['name'] in seen:
        [cdc_rdict.pop(key) for key in ['assembly_fasta', 'percent_reference_coverage', 'clearlabs_fasta', 'submission_id', 'terra_data_table_name', 'aligned_bai', 'aligned_bam', 'assembly_length_unambiguous', 'assembly_mean_coverage', 'consensus_flagstat', 'consensus_stats', 'vadr_alerts_list', 'vadr_num_alerts']]
        cdc_rdict = {k:(' ' if v=='' else v) for (k,v) in cdc_rdict.items()} # add whitespace in empty fields
        outrows.append(cdc_rdict)
    seen.add(row['name'])    
end = time.time()
logging.info(f"Finished {cdc_table_name} in {int(end-start)} seconds.")

# Now open and parse the LHJ County PHL Table
# Changing the prefix string when processing these next samples
logging.info("Beginning work on the LHJ tables.")
start = time.time()
prefix = "LHJ_"
lhj_table_name = "county_specimen_updated"
lhj_workspace_name = "dataAnalysis_SARS-CoV-2_County_Labs_Master"
# foi['clearlabs_fasta'] = ''
lhj_table = yoink_table(workspace_project, lhj_workspace_name, lhj_table_name)

for row in lhj_table:
    addFlag = False
    hasLhjClearlabs = False
    hasLhjFasta = False
    hasSubmissionId = False
    hasBeenMapped = False
    lhj_rdict = dict([('usherID', prefix+row['name']), ('name',row['name'])] + [(k, r) for k, r in foi.items()] +
              [(k,r) for k, r in row['attributes'].items() if k in foi.keys()])
    lhj_rdict['specimen_id'] = lhj_rdict['name']
    lhj_rdict['terra_data_table_name'] = lhj_table_name
    adict = copy.deepcopy(lhj_rdict)    
    aggregate.append(adict)   
    if re.search(r"fasta", lhj_rdict['clearlabs_fasta']):
        hasLhjClearlabs = True
    if re.search(r"fasta", lhj_rdict['assembly_fasta']):
        hasLhjFasta = True
    # We need to skip the rows from LosAngeles_County where the URL for their consensus.fasta file has been removed or modified
    if re.search(r'F50782|H49627|M24259|M24269|M24292|M24300|M24301|M25796|M25803|M25832|T1250|T1259|T1262|W22729', lhj_rdict['name']):
        continue
    # Skip specimens that lack both a sequencing_lab AND a collection_date
    if lhj_rdict['sequencing_lab'] == '' and lhj_rdict['collection_date'] == '':
        continue
    # First check if it has a gisaid_accession
    if re.search(r"EPI_ISL", lhj_rdict['gisaid_accession']):
        # Skip specimens with duplicated gisaid_accessions 
        if lhj_rdict['gisaid_accession'] in lhj_accession:
            continue
        else:
            lhj_accession.add(lhj_rdict['gisaid_accession'])    
        addFlag = True
        # test if it is in the epiToGisaid dict
        try:
           lhj_rdict['usherID'] = epiToGisaid[lhj_rdict['gisaid_accession']]
           hasBeenMapped = True
           if lhj_rdict['submission_id'] in epiToSubmission:
               hasSubmissionId = True
        except KeyError:
            try:
                lhj_rdict['usherID'] = epiToGisaid[mapping_table[lhj_rdict['gisaid_accession']]]
                hasBeenMapped = True
                if lhj_rdict['submission_id'] in epiToSubmission:
                    hasSubmissionId = True
            except KeyError:
                if re.search(r"fasta", lhj_rdict['assembly_fasta']) and lhj_rdict['percent_reference_coverage']!='':
                    noPassIds.append("{}\thas gisaid_accession but was not found in bigtree nightly build: QC issues most likely\n".format(lhj_rdict['name']))
                    continue
                else:
                    addFlag = False
                    noPassIds.append("{}\tno fasta file, and couldn't map mysterious gisaid_accession\n".format(lhj_rdict['name']))
                    continue
    elif lhj_rdict['gisaid_accession'] == '' or lhj_rdict['gisaid_accession'] == 'unknown':
        # This specimen_id has no gisaid_accession in Terra
        # Check if it has a subimssion_id in the epiToSubmission dict
        if lhj_rdict['submission_id'] in epiToSubmission:
            hasSubmissionId = True
            # Now check to see if this submission_id was already used by another specimen (it happens)
            if lhj_rdict['submission_id'] in submission_id_seen:
                hasSubmissionId = False
        # NOW check for the QC Threshold before deciding how to proceed
        # Specifically, do not assume that having a possible submission_id is going to be a panacea
        # There is all kinds of crap in Terra, especially in the county lhj workspace
        if hasLhjFasta or hasLhjClearlabs:
            if lhj_rdict['percent_reference_coverage'] == '':
                # failsQC
                noPassIds.append("{}\tapparently has a fasta file but the coverage field is empty or null\n".format(lhj_rdict['name'], lhj_rdict['percent_reference_coverage']))
                continue
            elif float(lhj_rdict['percent_reference_coverage']) < 70:
                # failsQC
                noPassIds.append("{}\tcoverage {} does not pass 70% threshold\n".format(lhj_rdict['name'], lhj_rdict['percent_reference_coverage']))
                continue
            else:
                # only gets this if it passed both the tests above.  Even if it has a submission_id, it can failQC
                addFlag = True
                if hasSubmissionId is True:
                    lhj_rdict['usherID'] = epiToSubmission[lhj_rdict['submission_id']]
                    hasBeenMapped = True
                elif hasLhjFasta:
                    # New Block of test code: do we have read permission for this specimen's fasta file?
                    if lhj_rdict['name'] not in has_fasta_permission:
                        permission = subprocess.run("gsutil -q stat " + lhj_rdict['assembly_fasta'], shell=True).returncode
                        if permission > 0:
                            no_permission.append(lhj_rdict['specimen_id'] + "\t" + lhj_rdict['sequencing_lab'] + "\t" + lhj_rdict['assembly_fasta'])
                            continue

                    lhjfastas.append(lhj_rdict['assembly_fasta'])
                    fasta_sources.append(lhj_rdict['sequencing_lab'] + "\t" + lhj_rdict['usherID'] + "\t" + lhj_rdict['assembly_fasta'])
                elif hasLhjClearlabs:
                    # New Block of test code: do we have read permission for this specimen's fasta file?
                    if lhj_rdict['name'] not in has_fasta_permission:
                        permission = subprocess.run("gsutil -q stat " + lhj_rdict['clearlabs_fasta'], shell=True).returncode
                        if permission > 0:
                            no_permission.append(lhj_rdict['specimen_id'] + "\t" + lhj_rdict['sequencing_lab'] + "\t" + lhj_rdict['clearlabs_fasta'])
                            continue

                    lhjfastas.append(lhj_rdict['clearlabs_fasta'])
                    fasta_sources.append(lhj_rdict['sequencing_lab'] + "\t" + lhj_rdict['usherID'] + "\t" + lhj_rdict['clearlabs_fasta'])
        else:
            continue

    if hasBeenMapped is True:
        # If the current specimen was successfully mapped but that strain is already being used by a different specimen
        # (don't ask, it happens) Then we are skipping this specimen, as wonderful as it might be.
        if lhj_rdict['usherID'] in mapped_usherIDs_or_strains:
            continue
        else:
            # However, if this is the very first time that we have successfully mapped this specific strain or usherID
            # Then add this information to the list to keep track of it
            mapped_usherIDs_or_strains.add(lhj_rdict['usherID'])

    if addFlag is True and not row['name'] in seen:
        if hasSubmissionId:
            submission_id_seen.add(lhj_rdict['submission_id'])
        [lhj_rdict.pop(key) for key in ['assembly_fasta', 'percent_reference_coverage', 'clearlabs_fasta', 'submission_id', 'terra_data_table_name', 'aligned_bai', 'aligned_bam', 'assembly_length_unambiguous', 'assembly_mean_coverage', 'consensus_flagstat', 'consensus_stats', 'vadr_alerts_list', 'vadr_num_alerts']]
        lhj_rdict = {k:(' ' if v=='' else v) for (k,v) in lhj_rdict.items()} # add whitespace in empty fields
        outrows.append(lhj_rdict)
    seen.add(row['name'])    
end = time.time()
logging.info(f"Finished {lhj_table_name} in {int(end-start)} seconds.")

# Now open and parse the VRDL Table
# Changing the prefix string when processing these next samples
logging.info("Beginning work on the VRDL tables' rows.")
start = time.time()
prefix = "VRDL_"
vrdl_table_name = "dashboard_specimen"
vrdl_workspace_name = "dataAnalysis_VRDL"
vrdl_table = yoink_table(workspace_project, vrdl_workspace_name, vrdl_table_name)

for row in vrdl_table:
    addFlag = False
    hasVrdlClearlabs = False
    hasVrdlFasta = False
    hasSubmissionId = False
    hasBeenMapped = False

    vrdl_rdict = dict([('usherID', prefix+row['name']), ('name',row['name'])] + [(k, r) for k, r in foi.items()] +
              [(k,r) for k, r in row['attributes'].items() if k in foi.keys()])
    vrdl_rdict['specimen_id'] = vrdl_rdict['name']
    vrdl_rdict['terra_data_table_name'] = vrdl_table_name
    adict = copy.deepcopy(vrdl_rdict)    
    aggregate.append(adict)   
    if re.search(r"fasta", vrdl_rdict['clearlabs_fasta']):
        hasVrdlClearlabs = True
    if re.search(r"fasta", vrdl_rdict['assembly_fasta']):
        hasVrdlFasta = True
    # Skip specimens that lack both a sequencing_lab AND a collection_date
    if vrdl_rdict['sequencing_lab'] == '' and vrdl_rdict['collection_date'] == '':
        continue
    # First check if it has a gisaid_accession
    if re.search(r"EPI_ISL", vrdl_rdict['gisaid_accession']):
        # Skip specimens with duplicated gisaid_accessions 
        if vrdl_rdict['gisaid_accession'] in vrdl_accession:
            continue
        else:
            vrdl_accession.add(vrdl_rdict['gisaid_accession'])    
        addFlag = True
        # test if it is in the epiToGisaid dict
        try:
           vrdl_rdict['usherID'] = epiToGisaid[vrdl_rdict['gisaid_accession']]
           hasBeenMapped = True
           if vrdl_rdict['submission_id'] in epiToSubmission:
               hasSubmissionId = True
        except KeyError:
            try:
                vrdl_rdict['usherID'] = epiToGisaid[mapping_table[vrdl_rdict['gisaid_accession']]]
                hasBeenMapped = True
                if vrdl_rdict['submission_id'] in epiToSubmission:
                    hasSubmissionId = True
            except KeyError:
                if (hasVrdlFasta or hasVrdlClearlabs) and vrdl_rdict['percent_reference_coverage']!='':
                    noPassIds.append("{}\thas gisaid_accession but was not found in bigtree nightly build: QC issues most likely\n".format(vrdl_rdict['name']))
                    continue
                else:
                    noPassIds.append("{}\tno fasta file, and couldn't map mysterious gisaid_accession\n".format(vrdl_rdict['name']))
                    continue
    elif vrdl_rdict['gisaid_accession'] == '' or vrdl_rdict['gisaid_accession'] == 'unknown':
        try: 
            vrdl_rdict['usherID'] = epiToSubmission[vrdl_rdict['submission_id']]
            hasBeenMapped = True
            hasSubmissionId = True
            addFlag = True
            if vrdl_rdict['submission_id'] in submission_id_seen:
                hasSubmissionId = False
        except KeyError:
            if hasVrdlFasta or hasVrdlClearlabs:
                if vrdl_rdict['percent_reference_coverage'] == '':
                    noPassIds.append("{}\tapparently has a fasta file but the coverage field is empty or null\n".format(vrdl_rdict['name'], vrdl_rdict['percent_reference_coverage']))
                    continue
                elif float(vrdl_rdict['percent_reference_coverage']) < 70:
                    noPassIds.append("{}\tcoverage {} does not pass 70% threshold\n".format(vrdl_rdict['name'], vrdl_rdict['percent_reference_coverage']))
                    continue
                else:
                    addFlag = True
                    if hasVrdlFasta:
                        # New Block of test code: do we have read permission for this specimen's fasta file?
                        if vrdl_rdict['name'] not in has_fasta_permission:
                            permission = subprocess.run("gsutil -q stat " + vrdl_rdict['assembly_fasta'], shell=True).returncode
                            if permission > 0:
                                no_permission.append(vrdl_rdict['specimen_id'] + "\t" + vrdl_rdict['sequencing_lab'] + "\t" + vrdl_rdict['assembly_fasta'])
                                continue

                        vrdlfastas.append(vrdl_rdict['assembly_fasta'])
                        fasta_sources.append(vrdl_rdict['sequencing_lab'] + "\t" + vrdl_rdict['usherID'] + "\t" + vrdl_rdict['assembly_fasta'])
                    elif hasVrdlClearlabs:
                        # New Block of test code: do we have read permission for this specimen's fasta file?
                        if vrdl_rdict['name'] not in has_fasta_permission:
                            permission = subprocess.run("gsutil -q stat " + vrdl_rdict['clearlabs_fasta'], shell=True).returncode
                            if permission > 0:
                                no_permission.append(vrdl_rdict['specimen_id'] + "\t" + vrdl_rdict['sequencing_lab'] + "\t" + vrdl_rdict['clearlabs_fasta'])                        
                                continue

                        vrdlfastas.append(vrdl_rdict['clearlabs_fasta'])
                        fasta_sources.append(vrdl_rdict['sequencing_lab'] + "\t" + vrdl_rdict['usherID'] + "\t" + vrdl_rdict['clearlabs_fasta'])
            else:
                continue

    if hasBeenMapped is True:
        # If the current specimen was successfully mapped but that strain is already being used by a different specimen
        # (don't ask, it happens) Then we are skipping this specimen, as wonderful as it might be.
        if vrdl_rdict['usherID'] in mapped_usherIDs_or_strains:
            continue
        else:
            # However, if this is the very first time that we have successfully mapped this specific strain or usherID
            # Then add this information to the list to keep track of it
            mapped_usherIDs_or_strains.add(vrdl_rdict['usherID'])

    if addFlag is True and not row['name'] in seen:
        if hasSubmissionId:
            submission_id_seen.add(vrdl_rdict['submission_id'])
        # } close if conditional
        [vrdl_rdict.pop(key) for key in ['assembly_fasta', 'percent_reference_coverage', 'clearlabs_fasta', 'submission_id', 'terra_data_table_name', 'aligned_bai', 'aligned_bam', 'assembly_length_unambiguous', 'assembly_mean_coverage', 'consensus_flagstat', 'consensus_stats', 'vadr_alerts_list', 'vadr_num_alerts']]
        vrdl_rdict = {k:(' ' if v=='' else v) for (k,v) in vrdl_rdict.items()} # add whitespace in empty fields
        outrows.append(vrdl_rdict)
    seen.add(row['name'])
end = time.time()
logging.info(f"Finished {vrdl_table_name} in {int(end-start)} seconds.")

now = subprocess.run("date", text=True, capture_output=True).stdout
logging.info(r"Finished at {now}")
# Create a new dict with the keys and values from the epiToGisaid dict
# Swapped with each other:
strainToAccession = {v: k for k, v in epiToGisaid.items()}

# Now iterate over the rows we want to print out to the samplemeta.tsv table, 
# and if we successfully mapped one of the samples from the public_meta table
# Then delete that entry from the strainToAccession dict.  This _should_ just
# leave the non-Terra, non-COVIDNet, California samples that will still be in
# The California Big Tree, as keys (strains) in the strainToAccession dict

for row in outrows:
    if row['usherID'] in strainToAccession:
        del strainToAccession[row['usherID']]

# Now print out this smaller version of the dict to a file to be used during 
# GISAID ingestion:
with open("strainToAccession.tsv", 'w') as outf:
    for key, value in strainToAccession.items():
        outf.write('%s\t%s\n' % (key, value))

# print full metadata file; it contains COVIDNet samples already in the tree as well as new ones
# Samples without gisaid_accessions that failQC for any reason are NOT added to to the samplemeta table
# This version also contains California samples that were sequenced by non-COVIDNet labs as part of a 
# CDC Initiative AND NOW: LHJ County PHL Samples as well that are also in Terra
# with open("prelim_samplemeta.tsv", 'wt') as outf:
with open("samplemeta.tsv", 'wt') as outf:
    fieldnames = ['usherID', 'name', 'pango_lineage', 'nextclade_clade', 'gisaid_accession', 'county', 'collection_date', 'paui', 'sequencing_lab', 'specimen_id', 'specimen_accession_number']
    writer = csv.DictWriter(outf, fieldnames=fieldnames, delimiter='\t', dialect=csv.unix_dialect, quoting=csv.QUOTE_MINIMAL)
    writer.writeheader()
    writer.writerows(outrows)
with open("aggregated_terra_table.tsv", 'wt') as outf:
    fieldnames = ['usherID', 'name', 'assembly_fasta', 'percent_reference_coverage', 'pango_lineage', 'nextclade_clade', 'gisaid_accession', 'county', 'collection_date', 'paui', 'sequencing_lab', 'submission_id', 'specimen_accession_number', 'clearlabs_fasta', 'specimen_id', 'terra_data_table_name', 'aligned_bai', 'aligned_bam', 'assembly_length_unambiguous', 'assembly_mean_coverage', 'consensus_flagstat', 'consensus_stats', 'vadr_alerts_list', 'vadr_num_alerts']
    writer = csv.DictWriter(outf, fieldnames=fieldnames, delimiter='\t', dialect=csv.unix_dialect, quoting=csv.QUOTE_MINIMAL)
    writer.writeheader()
    writer.writerows(aggregate)    
with open("failQC.tsv", 'wt') as outf:
    outf.writelines(noPassIds)
# fasta files for samples not already in the tree
with open("cdph_fafiles.txt", 'w') as outf:
    outf.write('\n'.join(cdphfastas))
with open("cdc_fafiles.txt", 'w') as outf:
    outf.write('\n'.join(cdcfastas))
with open("lhj_fafiles.txt", 'w') as outf:
    outf.write('\n'.join(lhjfastas))
with open("vrdl_fafiles.txt", 'w') as outf:
    outf.write('\n'.join(vrdlfastas))
with open ("fasta_file_sources.tsv", 'w') as outf:
    outf.write('\n'.join(fasta_sources))
with open ("no_permission_fasta_urls.tsv", 'w') as outf:
    outf.write('\n'.join(no_permission))    
CODE
    cut -f 2 "samplemeta.tsv" | tail -n +2 > "permitted_list.txt" 
    gsutil cp "permitted_list.txt" "gs://fc-secure-5bc717f3-d703-41ab-8521-a637292bca78/permitted_specimens.txt"
    >>>
    runtime {
      docker: "schaluvadi/pathogen-genomic-surveillance:api-wdl"
      memory: "128 GB"
      cpu: 8
      disks: "local-disk 375 SSD"
      maxRetries: 10
    }
    output{
        File cdphfastas = "cdph_fafiles.txt"
        File cdcfastas = "cdc_fafiles.txt"
        File lhjfastas = "lhj_fafiles.txt"
        File vrdlfastas = "vrdl_fafiles.txt"
        File noPassIds= "failQC.tsv"
        # File sample_meta = "prelim_samplemeta.tsv"
        File sample_meta = "samplemeta.tsv"
        File fasta_sources = "fasta_file_sources.tsv"
        File mapping_table = "epiToPublicAndDate.latest"
        File permitted_list = "permitted_list.txt"
        File no_permission = "no_permission_fasta_urls.tsv"
        File aggregate = "aggregated_terra_table.tsv"
        File non_terra_calif_samples = "strainToAccession.tsv"
    }
}
task gcs_copy_fafiles {
  meta { description: "Copied from https://github.com/broadinstitute/viral-pipelines/blob/master/pipes/WDL/tasks/tasks_terra.wdl#L3-L30" }
  input {
    File cdph_falist
    File cdc_falist
    File lhj_falist
    File vrdl_falist
    String  prefix
    Int num_threads = 16
    Int mem_size = 16
    Int diskSizeGB = 30
  }
  command <<<
    set -ux
    mkdir download_dir
    mkdir cdc_downloads
    mkdir lhj_downloads
    mkdir vrdl_downloads

    if [ -s ~{cdph_falist} ]
    then
        # copy the CDPH fasta files to the running instance
        cat ~{cdph_falist} | gsutil -m cp -I ./download_dir
        awk -F "/" '{print "download_dir/"$NF}' ~{cdph_falist} > cdph_fa
        split -l 500 cdph_fa cdph_fasplit
        for fname in $(ls cdph_fasplit*); do
            cat $(cat $fname) > combi.$fname &
        done
        wait
        for combi in $(ls combi.cdph_fasplit*); do
            cat $combi >> cdph_combined.fa
        done

        sed -i -E 's/>D([0-9])/>D-\1/' cdph_combined.fa
        sed -i 's/_redo//' cdph_combined.fa
        sed -i 's/_duplicated//' cdph_combined.fa
        sed -i "s/>/>~{prefix}/" cdph_combined.fa
    fi 

    if [ -s ~{cdc_falist} ]
    then
        # Now process the CA-CDC fasta files to their own directory
        cat ~{cdc_falist} | gsutil -m cp -I ./cdc_downloads
        awk -F "/" '{print "cdc_downloads/"$NF}' ~{cdc_falist} > cdc_fa
        split -l 500 cdc_fa cdc_fasplit
        for fname in $(ls cdc_fasplit*); do
            cat $(cat $fname) > cdc_combi.$fname &
        done
        wait
        for combi in $(ls cdc_combi.cdc_fasplit*); do
            cat $combi >> cdc_combined.fa
        done

        sed -i -E 's/>D([0-9])/>D-\1/' cdc_combined.fa
        sed -i 's/_redo//' cdc_combined.fa
        sed -i 's/_duplicated//' cdc_combined.fa
        # substitute the correct prefix for the CA-CDC files
        # to match the usherID in the samplemeta.tsv file
        sed -i "s/>/>CA_CDC_/" cdc_combined.fa
    fi
    
    if [ -s ~{lhj_falist} ]
    then
        # Now process the LHJ-County fasta files to their own directory
        cat ~{lhj_falist} | gsutil -m cp -I ./lhj_downloads
        awk -F "/" '{print "lhj_downloads/"$NF}' ~{lhj_falist} > lhj_fa
        split -l 500 lhj_fa lhj_fasplit
        for fname in $(ls lhj_fasplit*); do
            cat $(cat $fname) > lhj_combi.$fname &
        done
        wait
        for combi in $(ls lhj_combi.lhj_fasplit*); do
            cat $combi >> lhj_combined.fa
        done

        sed -i -E 's/>D([0-9])/>D-\1/' lhj_combined.fa
        sed -i 's/_redo//' lhj_combined.fa
        sed -i 's/_duplicated//' lhj_combined.fa
        # substitute the correct prefix for the LHJ-County files
        # to match the usherID in the samplemeta.tsv file
        sed -i "s/>/>LHJ_/" lhj_combined.fa
    fi

    if [ -s ~{vrdl_falist} ]
    then
        # Now process the VRDL fasta files to their own directory
        cat ~{vrdl_falist} | gsutil -m cp -I ./vrdl_downloads
        awk -F "/" '{print "vrdl_downloads/"$NF}' ~{vrdl_falist} > vrdl_fa
        split -l 500 vrdl_fa vrdl_fasplit
        for fname in $(ls vrdl_fasplit*); do
            cat $(cat $fname) > vrdl_combi.$fname &
        done
        wait
        for combi in $(ls vrdl_combi.vrdl_fasplit*); do
            cat $combi >> vrdl_combined.fa
        done

        sed -i -E 's/>D([0-9])/>D-\1/' vrdl_combined.fa
        sed -i 's/_redo//' vrdl_combined.fa
        sed -i 's/_duplicated//' vrdl_combined.fa
        # substitute the correct prefix for the VRDL files
        # to match the usherID in the samplemeta.tsv file
        sed -i "s/>/>VRDL_/" vrdl_combined.fa
    fi

    # Now concatenate the four sets of consensus.fasta together
    if [ -e "cdph_combined.fa" ]
    then
        cat cdph_combined.fa  >> combined.fa
    fi

    if [ -e "cdc_combined.fa" ]
    then
        cat cdc_combined.fa  >> combined.fa
    fi

    if [ -e "lhj_combined.fa" ]
    then
        cat lhj_combined.fa  >> combined.fa
    fi

    if [ -e "vrdl_combined.fa" ]
    then
        cat vrdl_combined.fa  >> combined.fa
    fi
    
  >>>
  output {
    File combined_fasta = "combined.fa"
  }
  runtime {
    docker: "quay.io/broadinstitute/viral-baseimage:0.1.20"
    cpu: num_threads
    memory: mem_size +" GB"
    disks: "local-disk " + diskSizeGB + " SSD"
  }
}

task getProblemVcf {
    meta { description: "Retrieves a regularly updated VCF with problematic sites in the SARS-CoV-2 sequence" }
    command {
        wget -O "problem.vcf" "https://raw.githubusercontent.com/W-L/ProblematicSites_SARS-CoV2/master/problematic_sites_sarsCov2.vcf"
        cp "/HOME/usher/test/NC_045512v2.fa" "NC_045512v2.fa"
    }
    output {
        File problem_vcf = "problem.vcf"
        File ref_fasta = "NC_045512v2.fa"
    }
    runtime {
        docker: "pathogengenomics/usher:latest"
    }
}

task mafft_align {
    meta {
        description: "Align multiple sequences from FASTA. Only appropriate for closely related (within 99% nucleotide conservation) genomes. See https://mafft.cbrc.jp/alignment/software/closelyrelatedviralgenomes.html"
    }
    input {
        File     sequences
        File     ref_fasta
        String   docker = "quay.io/broadinstitute/viral-phylo:2.1.19.1"
        Int      mem_size = 500
        Int      cpus = 64
    }
    command {
        set -e

        GENOMES="~{sequences}"

        # mafft align to reference in "closely related" mode
        mafft --auto --thread ~{cpus} --keeplength --addfragments $GENOMES ~{ref_fasta} > aligned.fasta

        # profiling and stats
        cat /proc/uptime | cut -f 1 -d ' ' > UPTIME_SEC
        cat /proc/loadavg > CPU_LOAD
        # cat /sys/fs/cgroup/memory/memory.max_usage_in_bytes > MEM_BYTES
    }
    runtime {
        docker: docker
        memory: mem_size + " GB"
        cpu :   cpus
        disks:  "local-disk 750 LOCAL"
        preemptible: 0
        dx_instance_type: "mem3_ssd1_v2_x36"
    }
    output {
        File   aligned_sequences = "aligned.fasta"
        # Int    max_ram_gb        = ceil(read_float("MEM_BYTES")/1000000000)
        Int    runtime_sec       = ceil(read_float("UPTIME_SEC"))
        String cpu_load          = read_string("CPU_LOAD")
    }
}

task faToVcf {
    meta { description: "uses UShER's faToVcf to turn a mafft multifasta alignment into VCF format" }
    input {
        File fasta
        File problem_vcf
        Int num_threads = 16
        Int mem_size = 500
        Int diskSizeGB = 30
    }
    command {
        # the reference sequence must be first
        faToVcf -maskSites=${problem_vcf} ${fasta} "sample.vcf"
    }
    output {
        File sample_vcf = "sample.vcf"
    }
    runtime {
        docker: "pathogengenomics/usher:latest"
        cpu: num_threads
        memory: mem_size +" GB"
        disks: "local-disk " + diskSizeGB + " SSD"
    }
}

task usherAddSamples {
    meta { description: "Runs UShER to create a new tree from an existing tree and a vcf file of input samples" }
    input {
        File vcf
        File protobuf
        Int mem_size = 30
        Int num_threads = 64
        Int diskSizeGB = 30
    }
    command {
        # Make paths files for Nextstrain clades and Pango lineages
        matUtils extract -i ${protobuf} -C clade-paths
        tail -n +2 clade-paths | grep -E '^[12]' | cut -f 1,3 > cladeToPath
        tail -n +2 clade-paths | grep -E '^[A-Za-z]' | cut -f 1,3 > lineageToPath

        # Make names files for Nextstrain clades and Pango lineages
        matUtils summary -i ${protobuf} -C sample-clades
        tail -n +2 sample-clades | perl -F"\t" -lane 'print "$F[1]\t$F[0]"' | sort > cladeToName
        tail -n +2 sample-clades | perl -F"\t" -lane 'print "$F[2]\t$F[0]"' | sort > lineageToName        

        usher-sampled -i ${protobuf} -T ${num_threads} -v ${vcf} -e 5 --optimization_radius 0 -o "ca_bigtree.pb"

        # Re-annotate ca_bigtree.pb
        matUtils annotate -i ca_bigtree.pb -l -P cladeToPath -c cladeToName -o "ca_bigtree.nextclade.pb"
        matUtils annotate -i ca_bigtree.nextclade.pb -P lineageToPath -c lineageToName -f 0.95 -o "new_tree.pb"

    }
    output {
        File new_tree = "new_tree.pb"
    }
    runtime { 
        docker: "pathogengenomics/usher-dev:latest"         
        cpu: num_threads
        memory: mem_size +" GB"
        disks: "local-disk " + diskSizeGB + " SSD"
    } 
}

task Extract {
    meta { description: "matUtils extracts all samples into (shared) subtrees; output table with metadata links to visualisation via nextstrain" }
    input {
	    File tree_pb
        File public_meta
        File samples_meta
        String prefix
        Int treesize
        String public_json_bucket
        Int num_threads = 80
        Int mem_size = 640
        Int diskSizeGB = 375
        File script
        # File genbank
    }
    String nextstr = "https://nextstrain.org/fetch/storage.googleapis.com"

    command <<<
        # pip install taxoniumtools

        wget -O "ncbiGenes.gtf.gz" "http://hgdownload.soe.ucsc.edu/goldenPath/wuhCor1/bigZips/genes/ncbiGenes.gtf.gz"
        gunzip ncbiGenes.gtf.gz
        cp "/HOME/usher/test/NC_045512v2.fa" "NC_045512v2.fa"

        # Adding the matUtils translation option to create an additional metadata table here
        matUtils summary --translate user_seqs.translation.tsv -i ~{tree_pb} -g ncbiGenes.gtf -f NC_045512v2.fa > translate

        cut -f1 ~{samples_meta} | tail -n +2 > sample.ids
        cut -f 1,8,9 ~{samples_meta} | tail -n +2 > name_and_paui.tsv

        # Now adding the translation table as an additional metadata file for subtree-assignment
        matUtils extract -M ~{public_meta},~{samples_meta},user_seqs.translation.tsv -i ~{tree_pb} -j ~{prefix} -s sample.ids -N ~{treesize}
 
        perl ~{script}

        for f in $(ls *_json_list.txt); 
        do
          echo "${f}" >> "${f}"
          tar -c -z -v -T "${f}" -f "${f}.tar.gz"
        done

        # adding these two lines for automated cluster detection to this existing 'Extract' task
        # tail -n +2 ~{samples_meta} | awk '{print $1"\tCalifornia"}' > california_samples.tsv
        # matUtils introduce -i ~{tree_pb} -s california_samples.tsv -o california_introductions.txt

        # profiling and stats
        cat /proc/uptime | cut -f 1 -d ' ' > UPTIME_SEC
        cat /proc/loadavg > CPU_LOAD
        # cat /sys/fs/cgroup/memory/memory.max_usage_in_bytes > MEM_BYTES

        matUtils extract -i ~{tree_pb} -l CDPH_BigTree.pb -g ncbiGenes.gtf -f NC_045512v2.fa -M ~{public_meta} > taxodium


        # use a subset of the fields in the table output by matUtils extract for conversion into JSON
        # Original version was prone to breaking when column output from matUtils changed:
        # cut -f 1,2,22 subtree-assignments.tsv > trimmed-assignments.tsv 
        # New version selects TSV columns based on field names
        awk -F"\t" -v cols=samples,json_file,paui,specimen_id,specimen_accession_number 'BEGIN {OFS = "\t";split(cols,out,",");printf "%s", out[1];for (i=2; i <= length(out); i++) {printf "%s%s", OFS, out[i];} print "";}NR==1 {for (i=1; i<=NF; i++) {ix[$i] = i }} NR>1 {for(i=1; i <= length(out); i++) {if ( i == 1 ) {printf "%s", $ix[out[i]];} else {printf "%s%s", OFS, $ix[out[i]];}} print "";}' subtree-assignments.tsv > trimmed-assignments.tsv

        # we can't have double slashes in URLs
        bucket=$(echo ~{public_json_bucket} | sed 's/\///')
        # turn the subtree file into a list of URLs in a html table
        outdir=$(date +~{prefix}_%m_%d_%Y)
        echo -n "$outdir" > subtree-folder-name.txt
        echo -n "${bucket}/$outdir" > resultbucket.txt
        urlstart="~{nextstr}/${bucket}/${outdir}/"
python3 - "$urlstart" <<CODE
import sys
import csv
import json

urlstart=sys.argv[1]
name_dict = dict()
lab_dict = dict()


with open('name_and_paui.tsv', newline='') as csvfile:
  tsvfile = csv.reader(csvfile, delimiter = '\t')
  for rows in tsvfile:
    name_dict[rows[0]] = rows[1]
    lab_dict[rows[0]] = rows[2]

with open('subtree-assignments.tsv', 'r') as f, open('paui2url.tsv', 'wt') as out_file, open('samples_subtrees.html', 'w') as o:
  tsv_writer = csv.writer(out_file, delimiter='\t')
  o.write('<html><body>')
  o.write('<table border=2 cellspacing=0 cellpadding=4>' + "\n")
  for line in f:
    items = line.split("\t")
    if items[1].endswith('json'):
        o.write('<tr>' + "\n")
        items[1] = "<a href=\"{0}{1}?s={2}\">{1}</a>".format(urlstart, items[1].split('/')[-1], items[0].replace('|','%7C'))
        urljson = items[1]
        sample = items[0]
        id = name_dict.get(sample, 'NONE')
        lab = lab_dict.get(sample, 'NONE')
        tsv_writer.writerow([lab, id, urljson])
    else:
        o.write('<tr bgcolor="#FAD7A0">' + "\n")
    for item in items:
        o.write("<td>%s</td>" % item)
    o.write('</tr>')
  o.write('</table></body></html>')

data = []
with open('trimmed-assignments.tsv', newline='') as csvFile:
    list_object = csv.DictReader(csvFile, delimiter = '\t')
    for row in list_object:
        data.append(row)
        dumped_object = json.dumps(list(list_object))
        with open('subtree-assignments.json', 'w', encoding = 'utf-8') as jsonf:
            jsonf.write(dumped_object)

CODE
    >>>	
    output {
        File out_html = "samples_subtrees.html"
        File out_tsv = "subtree-assignments.tsv"
        File specimen_mapped_to_paui = "name_and_paui.tsv"
        File paui_mapped_to_tree = "paui2url.tsv"
        Array[File] subtree_jsons = glob("*tar*")
        String bucket = read_string("resultbucket.txt")
        # Commenting this out for the time being on 2021-10-12
        # File node_table = "california_introductions.txt"
        File taxodium_file = "CDPH_BigTree.pb"
        File translation_table = "user_seqs.translation.tsv"
        File out_json = "subtree-assignments.json" 
        File subtree_folder = "subtree-folder-name.txt" 
        # Int    max_ram_gb        = ceil(read_float("MEM_BYTES")/1000000000)
        Int    runtime_sec       = ceil(read_float("UPTIME_SEC"))
        String cpu_load          = read_string("CPU_LOAD")
#         File tree_jsonl = "CDPH_BigTree.jsonl.gz"
    }
    runtime {
        # Previously, in 2022-07, this one was broken, dumped core (twice)
        # usher version: ? But trying it again now
        docker: "pathogengenomics/usher:latest"
        # Since JUL-2022, I was using this one with usher version 0.5.2(?):
        # docker: "yatisht/usher:latest"
        cpu: num_threads
        memory: mem_size +" GB"
        disks: "local-disk " + diskSizeGB + " SSD"
        maxRetries: 5
        cpuPlatform: "AMD Rome"
        zones: "us-central1-b"
    }   
}

task usher_to_taxonium {
    meta { description: "taxoniumtools converts the California Big Tree into a JSONL format compatible with the Taxonium Phylogenetic Tree Viewer" }
    input {
	    File protobuf
        File public_meta
        File genbank
        File overlay
    }

    command <<<
      pip install taxoniumtools

      usher_to_taxonium -i ~{protobuf} -o CDPH_BigTree.jsonl.gz -g ~{genbank} -m ~{public_meta} -c genbank_accession,date,country,Nextstrain_clade,pangolin_lineage,Nextstrain_clade_usher,pango_lineage_usher -t "California Big Tree Investigator" --overlay_html ~{overlay}

    >>>	
    output {
        File tree_jsonl = "CDPH_BigTree.jsonl.gz"
    }
    runtime {
        docker: "pathogengenomics/usher:latest"
        cpu: 32
        memory: "128 GB"
        maxRetries: 2
        cpuPlatform: "AMD Rome"
        zones: "us-central1-b"
    }
}
task prepareCountyData {
   input {
       File state_and_county_lex
       File public_meta
       File samples
       File airport_p
       File airport_c
    }
    command <<<
python3 <<CODE

# This script takes one or more metadata files and filters out samples that don't
#   match geographic names specified in a lexicon of place names. Use the lexicon
#   to store alternate spellings or capitalizatons of place names.
#   This script also creates files necessary for two regional analysis (one at the
#   CA county level and one at the US state level).
#
#   Arguments:
#      -lexiconfile: name of file with lexicon of place names
#      -metadatafiles: list of metadata files (CDPH metadata file should be listed first)
#      -extension: if using more than one geojson file this is list of file
#        name extensions to differentiate each set of files. Specify only the
#        file name extensions to use with the 2nd, 3rd, ..., set of files.
#      -isWDL: defalut is False. Set to true only if the script will be run 
#        as a WDL task in Terra.
#   Output files:
#      -metadata_merged.tsv: combined metadata file for both CDPH and public samples for the county analysis
#      -metadata_merged_us.tsv: combined metadata file for both CDPH and public samples for the state analysis
#      -sample_regions.tsv: list of sample names and corresponding regions for the county analysis
#      -sample_regions_us.tsv: list of sample names and corresponding regions for the state analysis
#      -sample_dates.tsv: list of sample names and corresponding dates for the county analysis
#      -sample_dates_us.tsv: list of sample names and corresponding dates for the state analysis
#      -pids.tsv: list of sample names and corresponding sample IDs/PAUIs
#
# Example command line usage:
#   python3 process_metadata.py -m public.plusGisaid.latest.metadata.tsv -mx samplemeta.tsv -l state_and_county_lexicon.txt
#-------------------------------------------------------------

import re, csv

lexiconfile = True
mfile = True
mfile_merge = True

## import sys, os #comment out for WDL 
#set path to directory 
## sys.path.insert(1, os.path.join(os.path.abspath(__file__ + "/../../../"), "src/python/")) #comment out for WDL

## Let me see, when you call this function, the default way it runs is, as a CLI Python
## script, i.e., where 'isWDL' = False
## Oh, the whole freaking python script is just this one single function, that is all there is
def process_metadata(lexiconfile, mfile, mfile_merge, extension=["_us"], isWDL = False):
    ## Then, in Jen's scheme of things, IF: you are running this code in some kind of 
    ## WDL setting, THEN: you need to manually uncomment the next line of code to 
    ## toggle the flag 'isWDL' to 'True', got it?
    #== for WDL ===
    isWDL = True
    #===

    ## So here we have two alternative states/options depending on whether or not
    ## the isWDL variable has been sent to to true
    ## See that?  These funkogenic tilde + curly braces variable interpolation WDL code things
    ## Don't even get seen/evaluated, UNLESS you are definitely visiting WDL-land
    if isWDL:
        lexiconfile = '~{state_and_county_lex}'
        mfile = '~{public_meta}'
        mfile_merge = '~{samples}'
        ext = "_us" 
        #names of airport sample input files
        airport_file_p = '~{airport_p}'
        airport_file_c = '~{airport_c}'
    else:
        ## Now, here we are in this block which is basically the DEFAULT code that gets
        ## executed, UNLESS you toggled isWDL to 'True'
        ## I find this confusing, why would mfile_merge _EVER_ be a list?
        ## Isn't it a big TSV table? No, my brain hurts, it must be getting too late
        ## Isn't the file named mfile_merge actually the OUTPUT from this whole script?!
        ## Clearly I am confused or not thinking it through clearly
        if type(mfile_merge) == list:
            mfile_merge = mfile_merge[0]
        ## Likewise, what the fuck is this bit doing?!  Why would you set the
        ## ext variable to _us?  Oh, wait (maybe?) It may be that None is a little bit
        ## Like undef in Perl?  Nope I am still totally unclear about this and how it works.
        ## Fuck my Literal Life.
        if extension is None:
            ext = "_us"
        else:
            ext = extension[0]
        #names of airport sample input files
        airport_file_p = "F1a-qry-AirportCOVIDNet-ToUCSC-Data-P-ALL.csv"
        airport_file_c = "F1b-qry-AirportCOVIDNet-ToUCSC-Data-C-ALL.csv"

    county_conversion = {}
    state_conversion = {}
    airp_conversion = {}
    ## my thoughts on this next block of code:
    ## IF: you specified a file name on the command line, probably with some -l thing, or whatever
    ## then this variable is carried or specified by that, somehow.  Otherwise, you are in WDL-land
    ## And we already know the content in this variable was set the WDL way.  Does that sound right?
    with open(lexiconfile) as inf:
        for entry in inf:
            spent = entry.strip().split(",")
            if "Airport" in spent[0]:
                airp_conversion[spent[1]] = spent[0]
            elif "County" in spent[0]:
                county_conversion[spent[1].upper()] = spent[0]
            else:
                for alternative in spent:
                    state_conversion[alternative.upper()] = spent[0] 

    metadata = open("metadata_merged.tsv","w+") #output file for merged metadata, for US + CA county analysis
    badsamples = open("rejected_samples.txt","w+") # file for rejected sample names
    region_assoc = open("sample_regions.tsv","w+") # file to store associations between sample ID and region name, for US + CA county analysis
    region_assoc_us = open("sample_regions" + ext + ".tsv","w+") # file to store associations between sample ID and region name, for US analysis
    date_file = open("sample_dates.tsv","w+") # file to store associations between sample ID and sample dates, for US + CA county analysis
    date_file_us = open("sample_dates" + ext + ".tsv","w+") # file to store associations between sample ID and sample dates, for US analysis
    print("sample_id\tdate", file = date_file)
    print("sample_id\tdate", file = date_file_us)
    pid_assoc = open("pids.tsv","w+") # file to store associations between sample ID and specimen_id (formerly PAUI or link_id)
    date_pattern = '[0-9]{4}-[0-9]{2}-[0-9]{2}'
    #write metadata header
    print("strain\tname\tpango_lineage\tnextstrain_clade\tgisaid_accession\tcounty\tdate\tpaui\tsequencing_lab\tspecimen_id\tspecimen_accession_number\tgenbank_accession\tcountry", file = metadata)
    duplicates = set() #stores sample names of potential duplicates

    #read airport sample data
    airport_data = []
    with open(airport_file_p, mode='r') as csv_file:
        data = csv.DictReader(csv_file)
        for row in data:
            airport_data.append([str(row["Barcode"]),'',row["GISAID_epi_isl"],row["Kiosk"],row["Collection_Date"]])
    with open(airport_file_c, mode='r') as csv_file:
        data = csv.DictReader(csv_file)
        for row in data:
            airport_data.append([str(row["Submitter Specimen ID"]),str(row["PAUI"]),row["GISAID_epi_isl"],row["Airport"],row["Collection_Date"]])
    
    with open(mfile_merge) as inf:
        print("parsing input metadata file: " + mfile_merge)
        # CA metadata header: usherID,name,pango_lineage,nextclade_clade,gisaid_accession,county,collection_date,paui,sequencing_lab,specimen_id,specimen_accession_number 
        fields = inf.readline() #skip header
        for entry in inf:
            has_valid_county = False
            fields = entry.split("\t")
            for i in range(len(fields)):
                fields[i] = fields[i].strip()
            fields.append("") #genbank_accession
            fields.append("USA") #country
            
            #First, check to see if item is in airports file. If so, override county region with airport region
            # and save values to fill in any missing blanks in metadata
            ids = [fields[0],fields[1],fields[7],fields[-2],fields[-1]]
            isAirport = False
            aiport_paui = ""
            airport_gisaid = ""
            airport_abbr = ""
            airport_date = ""
            for item in airport_data:
                if any(item[0] in id for id in ids):
                    isAirport = True
                    airport_paui = item[1]
                    airport_gisaid = item[2]
                    airport_abbr = item[3]
                    airport_date = item[4]
                    break
                elif item[1] != "" and any(item[1] in id for id in ids):
                    isAirport = True
                    airport_paui = item[1]
                    airport_gisaid = item[2]
                    airport_abbr = item[3]
                    airport_date = item[4]
                    break
                elif item[2] != "" and item[2] == fields[4]:
                    isAirport = True
                    airport_paui = item[1]
                    airport_gisaid = item[2]
                    airport_abbr = item[3]
                    airport_date = item[4]
                    break
            #if date isn't valid, reset to blank to avoid having to do multiple validity checks
            if airport_date != "" and not re.search(date_pattern, airport_date):
                airport_date = ""

            #Next, assign a region
            #assign region for airport data
            if isAirport:
                if airport_abbr in airp_conversion:
                    text = airp_conversion[airport_abbr].replace(" ", "_")
                    print(fields[0] + "\t" + text, file = region_assoc)
                    print(fields[0] + "\t" + text, file = region_assoc_us)
                else:
                    isAirport = False
            #assign region for non-airport data
            if not isAirport:
                #assign to CA state in region file
                print(fields[0] + "\tCalifornia", file = region_assoc_us)
                #check if county is in lexicon and if so add to region association file
                county = fields[5]
                if county != "":
                    if county.upper() in (c.upper() for c in county_conversion): # convert all names to uppercase to avoid differences in case
                        has_valid_county = True
                        text = county_conversion[county.upper()].replace(" ", "_")
                        print(fields[0] + "\t" + text, file = region_assoc)
            
            #Next, add dates to date file if needed
            #add sample names to list to check for duplicates; add date to date file
            if fields[0].startswith("USA/"):
                #add sample name if needed to check for duplicates later on
                duplicates.add(fields[0])
                #check for valid date in file name, use date field if invalid
                if not re.search(date_pattern, fields[0][-10:]):
                    if re.search(date_pattern, fields[6]):
                        print(fields[0] + "\t" + fields[6], file = date_file_us)
                        if has_valid_county or isAirport:
                            print(fields[0] + "\t" + fields[6], file = date_file)
                    elif isAirport and airport_date != "":
                        print(fields[0] + "\t" + airport_date, file = date_file)
                        print(fields[0] + "\t" + airport_date, file = date_file_us)
            else:
                #check for valid date and add sample ID and date to sample dates file
                if re.search(date_pattern, fields[6]):
                    print(fields[0] + "\t" + fields[6], file = date_file_us)
                    if has_valid_county or isAirport:
                        print(fields[0] + "\t" + fields[6], file = date_file)
                elif isAirport and airport_date != "":
                    print(fields[0] + "\t" + airport_date, file = date_file)
                    print(fields[0] + "\t" + airport_date, file = date_file_us)
            
            #Now, add Specimen ID to association file
            if fields[9] != "":
                print(fields[0] + "\t" + fields[9], file = pid_assoc)

            #Finally, write item to merged metadata file
            if isAirport:
                #if date, gisaid id, or paui are missing, fill those in
                if fields[6] == "" and airport_date != "":
                    fields[6] = airport_date
                if fields[7] == "" and aiport_paui != "":
                    fields[7] = aiport_paui
                if fields[4] == "" and airport_gisaid != "":
                    fields[4] = airport_gisaid
            print("\t".join(fields), file = metadata)


    with open(mfile) as inf:
        print("parsing input metadata file: " + mfile)
        # public metadata header: strain,genbank_accession,date,country,host,completeness,length,Nextstrain_clade,pangolin_lineage,Nextstrain_clade_usher,pango_lineage_usher
        fields = inf.readline() #skip header
        for entry in inf:
            fields = entry.strip().split("\t")
            country = fields[0].split("/")[0]
            #first, remove duplicate CDPH data
            if country == "USA" and fields[0] in duplicates:
                print(fields[0], file = badsamples)
            else:
                #add item to merged metadata file
                newfields = []
                newfields.append(fields[0]) #sample name
                newfields.append("") #CDPH name
                newfields.append(fields[10]) #pango_lineage_usher (public metadata); pango_lineage (Covidnet)
                newfields.append(fields[9]) #Nextstrain_clade_usher (public metadata); nextclade_clade (Covidnet)
                newfields.append("") #gisaid_accession
                newfields.append("") #county name
                newfields.append(fields[2]) #date
                newfields.append("") #paui
                newfields.append("") #sequencing_lab
                newfields.append("") #specimen_id
                newfields.append("") #specimen_accession_number
                newfields.append(fields[1]) #genbank_accession
                newfields.append(fields[3]) #country
                print("\t".join(newfields), file = metadata)
                # get region for US samples
                if country == "USA":
                    state = fields[0].split("/")[1].split("-")[0]
                    if state in state_conversion:
                        #add sample ID and state name to sample regions files
                        text = state_conversion[state.upper()].replace(" ", "_")
                        print(fields[0] + "\t" + text, file = region_assoc_us)
                        #filter out CA samples for CA county analysis
                        if state != "CA":
                            print(fields[0] + "\t" + text, file = region_assoc)

    metadata.close()
    badsamples.close()
    region_assoc.close()
    date_file.close()
    pid_assoc.close()
    region_assoc_us.close()
    date_file_us.close()

process_metadata(lexiconfile,mfile,mfile_merge)

CODE
    >>>
    output {
       File merged = "metadata_merged.tsv"
       File regions = "sample_regions.tsv"
       File pids = "pids.tsv"
       File date_file = "sample_dates.tsv"
       File regions_us = "sample_regions_us.tsv"
       File date_file_us = "sample_dates_us.tsv"
    }
    runtime {
        docker: "pathogengenomics/usher:latest" 
        cpu: 32
        memory: "128 GB"
        maxRetries: 2
     }   
}

task introduce {
    input { 
       File cleaned
       File regions
       File date_file
       Int num_threads = 80
       Int mem_size = 640
       Int diskSizeGB = 375
    }
    command <<<
      matUtils introduce -i ~{cleaned} -s ~{regions} -M ~{date_file} -u hardcoded_clusters.tsv -X 2
    >>>
    output {
       File clusters = "hardcoded_clusters.tsv"
    }
    runtime {
        docker: "pathogengenomics/usher:latest"
        cpu: num_threads
        memory: mem_size +" GB"
        disks: "local-disk " + diskSizeGB + " SSD"
        maxRetries: 2
        cpuPlatform: "AMD Rome"
        zones: "us-central1-b"
    }
}
task introduce_states {
    input { 
       File cleaned_us
       File regions_us
       File date_file_us
       Int num_threads = 80
       Int mem_size = 640
       Int diskSizeGB = 375
    }
    command <<<
      matUtils introduce -i ~{cleaned_us} -s ~{regions_us} -M ~{date_file_us} -u hardcoded_clusters_us.tsv -X 2
    >>>
    output {
       File clusters_us = "hardcoded_clusters_us.tsv"
    }
    runtime {
        docker: "pathogengenomics/usher:latest"
        cpu: num_threads
        memory: mem_size +" GB"
        disks: "local-disk " + diskSizeGB + " SSD"
        maxRetries: 2
        cpuPlatform: "AMD Rome"
        zones: "us-central1-b"
    }
}
task updateJavascript {
     input {
        File state_and_county_lex
        File us_states_w_ca_counties_geo
        File clusters_counties
        File us_states_geo
        File clusters_states
     }
     command <<<
       # Original command contained an error, but mysteriously worked?
       # pip install dateutil
       pip install python-dateutil
       
python3 <<CODE

import json
import math      
import datetime as dt
from dateutil.relativedelta import relativedelta
import logging

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s', filename='Python_logfile.log', filemode='w')
logging.getLogger('chardet').setLevel(logging.CRITICAL+1)

def read_lexicon(lfile):
   conversion = {}
   with open(lfile) as inf:
      for entry in inf:
         spent = entry.strip().split(",")
         for alternative in spent:
            conversion[alternative] = spent[0]
            # automatically create an all uppercase lexicon alternative
            if alternative != alternative.upper():
               conversion[alternative.upper()] = spent[0]
   return conversion

def validate_geojson(gfile):
   f = open(gfile)
   geojson_lines = json.load(f)
   f.close()
   for feature in geojson_lines["features"]:
      if "name" in feature["properties"]:
         return 1
      else:
         return 0

target = ['~{us_states_w_ca_counties_geo}' , '~{us_states_geo}']
conversion = read_lexicon('~{state_and_county_lex}')
extension = ["", "_us"]

def update_js(target=[''], conversion = {}, extension=['']):
    if validate_geojson('~{us_states_w_ca_counties_geo}') != 1:
       print("GeoJSON file DOES NOT have 'name' field")
       sys.exit()

    cluster_file = ['~{clusters_counties}', '~{clusters_states}']
    monthswap = {"Jan":"01","Feb":"02","Mar":"03","Apr":"04","May":"05","Jun":"06","Jul":"07","Aug":"08","Sep":"09","Oct":"10","Nov":"11","Dec":"12"}
    conversion.update({v:v for k,v in conversion.items()})
    conversion["indeterminate"] = "indeterminate"
    datepoints = ["all", dt.date.today()-relativedelta(months=12), dt.date.today()-relativedelta(months=6), dt.date.today()-relativedelta(months=3)]
    for idx, gfile in enumerate(target):
        svd = {"type":"FeatureCollection", "features":[]}
        # here, the data is stored in a series of dictionaries
        # which are generally structured with the minimum date as the outermost layer
        # then the destination of an introduction
        # then the source of an introduction
        prefd = {datepoints[0]:"", datepoints[1]:"12_", datepoints[2]:"6_", datepoints[3]:"3_"}
        dinvc = {d:{} for d in datepoints}
        dsvc = {d:{} for d in datepoints}
        dotvc = {d:{} for d in datepoints}
        dovc = {d:{} for d in datepoints}
        with open(cluster_file[idx]) as inf:
           for entry in inf:
              spent = entry.strip().split("\t")
              if spent[0] == "cluster_id":
                 continue
              reg = conversion[spent[9].replace("_"," ")]
              if "indeterminate" in spent[10]:
                 continue
              # get the date of this cluster's earliest sample into a usable form
              dsplt = spent[2].split("-")
              if dsplt == "no-valid-date".split("-"):
                 cdate = dt.date(year=2019,month=11,day=1)
              else:
                 cdate = dt.date(year=int(dsplt[0]), month=int(monthswap[dsplt[1]]), day=int(dsplt[2]))
              for startdate, ovc in dovc.items():
                 if startdate == "all" or cdate > startdate:
                    if reg not in dsvc[startdate]:
                       dsvc[startdate][reg] = 0
                    dsvc[startdate][reg] += spent[-1].count(',') + 1
                    if reg not in dinvc[startdate]:
                       dinvc[startdate][reg] = 0
                    dinvc[startdate][reg] += 1
                    otvc = dotvc[startdate]
                    ovc = dovc[startdate]
                    if reg not in ovc:
                       ovc[reg] = {}
                    for tlo in spent[10].split(","):
                       orig = conversion[tlo.replace("_"," ")]
                       if orig not in otvc:
                          otvc[orig] = 0
                       otvc[orig] += 1
                       if orig not in ovc[reg]:
                          ovc[reg][orig] = 0
                       ovc[reg][orig] += 1
        dsumin = {sd:sum(invc.values()) for sd,invc in dinvc.items()}
        sids = {}
        f = open(gfile)
        geojson_lines = json.load(f)
        f.close()
        id = 0
        # we fill in the basic count of introductions to each area first
        # as well as fill in an integer "ID" if its not already present
        for data in geojson_lines["features"]:
            data["properties"]["intros"] = {}
            for sd, invc in dinvc.items():
                prefix = prefd[sd]
                data["properties"]["intros"][prefix + "basecount"] = invc.get(data["properties"]["name"],0) 
            svd["features"].append(data)
            if "id" in data:
                sids[data["properties"]["name"]] = data["id"]
            else:
                data["id"] = str(id)
                sids[data["properties"]["name"]] = str(id)
                id += 1
        # update the data intros list with specific state values
        for ftd in svd["features"]:
            # update the ftd["properties"]["intros"] with each state
            # state introductions to itself, for now, I will fill with indeterminate
            # this is transposed so that the introductions to each state are stored across each other state by origin
            # in order that coloring and hovertext can be correctly accessed.
            iid = ftd['properties']["name"]
            did = {}
            # for timeslice
            for sd, ovc in dovc.items():
                # get everything where this specific row/region is an origin
                prefix = prefd[sd]
                # fill with 0
                inv_ovc = {k:subd.get(iid,0) for k,subd in ovc.items()}
                for destination, count in inv_ovc.items():
                    # scale the count for display
                    if destination == "indeterminate":
                        continue
                    did = sids[conversion.get(destination,destination)]
                    ftd["properties"]["intros"][prefix + "raw" + did] = count
                    if count > 0: #debug: if count > 5:
                        sumin = dsumin[sd]
                        invc = dinvc[sd]
                        otvc = dotvc[sd]
                        ftd["properties"]["intros"][prefix + did] = math.log10(count * sumin / invc[destination] / otvc[iid])
                    else:
                        # if there are less than 5 counts, the log correction can do some pretty extreme highlighting
                        # for example, even a single introduction between two distant places may be surprising
                        # but that doesn't mean it should get a lot of emphasis. So we cut off anything with less than 5 introductions total.
                        ftd["properties"]["intros"][prefix + did] = -0.5
        region_file = "regions" + extension[idx] + ".js"
        with open(region_file,"w") as outf:
            print("//data updated via update_js.py",file=outf)
            if idx == 0:
                print('var None = "None";',file=outf)
            print('var introData' + extension[idx] + ' = {"type":"FeatureCollection","features":[',file=outf)
            for propd in svd['features']:
                assert "intros" in propd["properties"]
                print(str(propd) + ",",file=outf)
            print("]};",file=outf)

update_js(target, conversion, extension)

CODE
     >>>
     output {
       File js_regions = "regions.js"
       File js_regions_us = "regions_us.js"
       File Python_log = "Python_logfile.log"
     }
     runtime {
        docker: "schaluvadi/pathogen-genomic-surveillance:api-wdl"
        cpu: 32
        memory: "128 GB"
        # maxRetries: 2
     }   
   }

task generateDisplayTables {
     input {
        File clusters_counties
        File county_pids 
     } 
     command <<<
python3 <<CODE

import json, gzip

# For CDPH data, returns an array containing the unique sample ID's for
# each sample in a cluster
def get_sample_pauis(items,pid_assoc):
    pids_arr = []
    samples = items.split(",")
    for s in samples:
        # get PAUI's from sample names
        if s in pid_assoc:
            pids_arr.append(pid_assoc[s])
    return pids_arr

# set flag to identify whether to add custom fields to output cluster files
is_custom = True

# function to add quotes around a variable for JSON formatting
def addq(item):
    return "\"" + item + "\""

def fix_month(datestr):
    monthswap = {"Jan":"01","Feb":"02","Mar":"03","Apr":"04","May":"05","Jun":"06","Jul":"07","Aug":"08","Sep":"09","Oct":"10","Nov":"11","Dec":"12"}
    splitr = datestr.split("-")
    return splitr[0] + "-" + monthswap.get(splitr[1],splitr[1]) + "-" + splitr[2]

# get sample name/PAUI associations
if is_custom:
    pid_assoc = {} # links sample ID with unique CDPH ID (e.g, PAUI, specimen_id)
    with open('~{county_pids}') as inf:
        for entry in inf:
            spent = entry.strip().split("\t")
            pid_assoc[spent[0]] = spent[1]

if is_custom:
   sample_pids = {} # stores association between cluster ID and PAUIs

# get clusters data and put into array
cluster_data = []
bad_date_data = [] #store clusters with no-valid-date
with open('~{clusters_counties}') as inf:
    for entry in inf:
        spent = entry.strip().split("\t")
        if spent[0] == "cluster_id": 
            continue
        if is_custom:
            # add California sample PAUIs
            ids = ""
            pids_arr = get_sample_pauis(spent[-1],pid_assoc)
            if pids_arr:
                sample_pids[spent[0]] = pids_arr
                ids = ",".join(pids_arr)
            spent.append(ids)
        #add additional field to handle asterisked growth values
        spent.append(spent[4])
        #check for cluster with no-valid-dates
        if spent[2] == "no-valid-date" and spent[3] == "no-valid-date":
            # add asterisk on growth values and put into separate array
            spent[-1] = spent[-1] + "*"
            bad_date_data.append(spent)
        else:
            #fix date format
            spent[2] = fix_month(spent[2])
            spent[3] = fix_month(spent[3])
            if int(spent[1]) <= 5:
                # add asterisk on growth value
                spent[-1] = spent[-1] + "*"
            cluster_data.append(spent)

# now, sort by growth score
cluster_data.sort(key = lambda x: x[4], reverse = True)
# sort clusters with no-valid-date by growth score and append to cluster_data at the end
bad_date_data.sort(key = lambda x: x[4], reverse = True)
cluster_data.extend(bad_date_data)

#output data to be compatible with parse.JSON
# -create as compact a string as possible,
# -only add quotes to items that are strings to save space
txt_data = "["
txt_samples = "["
for i, d in enumerate(cluster_data):
    outline_data = [addq(d[0]), addq(d[9]), d[1], addq(d[2]), addq(d[3]), addq(d[12]), addq(d[13]), addq(d[10]), d[11], addq(d[-1])]
    outline_samples = [addq(d[15])]
    if is_custom:
        # get the number of PAUIS
        npids = "0"
        if d[16] != "":
            npids = str(d[16].count(',') + 1)
        outline_data.append(npids)
        outline_samples.append(addq(d[16]))
    txt_data += "[" + ",".join(outline_data) + "]"
    txt_samples += "[" + ",".join(outline_samples) + "]"
    if i == len(cluster_data)-1:
        txt_data += "]"
        txt_samples += "]"
    else:
        txt_data += ","
        txt_samples += ","
   
# now write data to file, and gzip for quicker loading into browser
# basic cluster data (no samples) 
with gzip.open("cluster_data.json.gz", "wb") as f:
    f.write(txt_data.encode())
# sample names for each cluster
with gzip.open("sample_data.json.gz", "wb") as f:
    f.write(txt_samples.encode())


# write cluster id and specimen ids for Investigator
if is_custom:
    with open("cluster_pids.json","w+") as outf:
        print(json.dumps(sample_pids).replace(" ", ""), file = outf)
CODE
     >>>
     output {
       File cluster_data = "cluster_data.json.gz"
       File sample_data = "sample_data.json.gz"
       File pids = "cluster_pids.json"
     }
     runtime {
        docker: "pathogengenomics/usher:latest" 
        cpu: 32
        memory: "128 GB"
        maxRetries: 2
     }   
}
task generateDisplayTables_states {
     input {
        File clusters_states
        File state_pids 
     } 
     command <<<
python3 <<CODE

import json, gzip

# For CDPH data, returns an array containing the unique sample ID's for
# each sample in a cluster
def get_sample_pauis(items,pid_assoc):
    pids_arr = []
    samples = items.split(",")
    for s in samples:
        # get PAUI's from sample names
        if s in pid_assoc:
            pids_arr.append(pid_assoc[s])
    return pids_arr

# set flag to identify whether to add custom fields to output cluster files
is_custom = True



# function to add quotes around a variable for JSON formatting
def addq(item):
    return "\"" + item + "\""

def fix_month(datestr):
    monthswap = {"Jan":"01","Feb":"02","Mar":"03","Apr":"04","May":"05","Jun":"06","Jul":"07","Aug":"08","Sep":"09","Oct":"10","Nov":"11","Dec":"12"}
    splitr = datestr.split("-")
    return splitr[0] + "-" + monthswap.get(splitr[1],splitr[1]) + "-" + splitr[2]

# get sample name/PAUI associations
if is_custom:
    pid_assoc = {} # links sample ID with unique CDPH ID (e.g, PAUI, specimen_id)
    with open('~{state_pids}') as inf:
        for entry in inf:
            spent = entry.strip().split("\t")
            pid_assoc[spent[0]] = spent[1]

if is_custom:
   sample_pids = {} # stores association between cluster ID and PAUIs

# get clusters data and put into array
cluster_data = []
bad_date_data = [] #store clusters with no-valid-date
with open('~{clusters_states}') as inf:
    for entry in inf:
        spent = entry.strip().split("\t")
        if spent[0] == "cluster_id": 
            continue
        if is_custom:
            # add California sample PAUIs
            ids = ""
            pids_arr = get_sample_pauis(spent[-1],pid_assoc)
            if pids_arr:
                sample_pids[spent[0]] = pids_arr
                ids = ",".join(pids_arr)
            spent.append(ids)
        #add additional field to handle asterisked growth values
        spent.append(spent[4])
        #check for cluster with no-valid-dates
        if spent[2] == "no-valid-date" and spent[3] == "no-valid-date":
            # add asterisk on growth values and put into separate array
            spent[-1] = spent[-1] + "*"
            bad_date_data.append(spent)
        else:
            #fix date format
            spent[2] = fix_month(spent[2])
            spent[3] = fix_month(spent[3])
            if int(spent[1]) <= 5:
                # add asterisk on growth value
                spent[-1] = spent[-1] + "*"
            cluster_data.append(spent)

# now, sort by growth score
cluster_data.sort(key = lambda x: x[4], reverse = True)
# sort clusters with no-valid-date by growth score and append to cluster_data at the end
bad_date_data.sort(key = lambda x: x[4], reverse = True)
cluster_data.extend(bad_date_data)

#output data to be compatible with parse.JSON
# -create as compact a string as possible,
# -only add quotes to items that are strings to save space
txt_data = "["
txt_samples = "["
for i, d in enumerate(cluster_data):
    outline_data = [addq(d[0]), addq(d[9]), d[1], addq(d[2]), addq(d[3]), addq(d[12]), addq(d[13]), addq(d[10]), d[11], addq(d[-1])]
    outline_samples = [addq(d[15])]
    if is_custom:
        # get the number of PAUIS
        npids = "0"
        if d[16] != "":
            npids = str(d[16].count(',') + 1)
        outline_data.append(npids)
        outline_samples.append(addq(d[16]))
    txt_data += "[" + ",".join(outline_data) + "]"
    txt_samples += "[" + ",".join(outline_samples) + "]"
    if i == len(cluster_data)-1:
        txt_data += "]"
        txt_samples += "]"
    else:
        txt_data += ","
        txt_samples += ","
   
# now write data to file, and gzip for quicker loading into browser
# basic cluster data (no samples) 
with gzip.open("cluster_data_us.json.gz", "wb") as f:
    f.write(txt_data.encode())
# sample names for each cluster
with gzip.open("sample_data_us.json.gz", "wb") as f:
    f.write(txt_samples.encode())


# write cluster id and specimen ids for Investigator
if is_custom:
    with open("cluster_pids_us.json","w+") as outf:
        print(json.dumps(sample_pids).replace(" ", ""), file = outf)
CODE
     >>>
     output {
       File cluster_data_us = "cluster_data_us.json.gz"
       File sample_data_us = "sample_data_us.json.gz"
       File pids_us = "cluster_pids_us.json"
     }
     runtime {
        docker: "pathogengenomics/usher:latest" 
        cpu: 32
        memory: "128 GB"
        maxRetries: 2
     }   
}
task prepareTaxonium {
    input {
        File clusters # hardcoded_clusters.tsv from the introduce task, 16 columns
        File regions  # sample_regions.tsv from the prepareCountyData task, 2 columns
        File merged   # metadata_merged.tsv from the prepareCountyData task, 13 columns
     }
     command <<<
python3 <<CODE
# this data structure is python set
# for samples(?)
sd = {}
with open('~{clusters}') as inf:
    for entry in inf:
        # First any leading or trailing whitespace on the entire line is removed, 
        # Then the line is split on TABs to create a list of fields
        spent = entry.strip().split("\t")
        # Test if this incoming row is the header row
        # Skip this row if it is the header
        if spent[0] == 'cluster_id':
            continue
        # for each data row
        # take the last, or final, or terminal field, which is column 16, 
        # named 'samples' and split it on commas, creating an
        # anonymous (unnamed) list of samples
        for s in spent[-1].split(","):
            # now create a new entry in the sent where the samplename is the key
            # and the cluster_id is the value.  Since it is a set, there
            # cannot be any duplicates, by which I mean if there are any duplicates then 
            # they will get overwritten, right?
            sd[s] = spent[0] # sd[sample name] = cluster id
# Another python set data structure
# I guess this one is for regions            
rd = {} 
with open('~{regions}') as inf:
    for entry in inf:
        # same logic as above except this two column TSV table
        # does not have a header
        spent = entry.strip().split("\t")
        # So really it should be the same samples now with a region
        # UNLESS there are samples in hardcoded_clusters.tsv that are NOT
        # in sample_regions.tsv
        rd[spent[0]] = spent[1] # rd[sample name] = region
with open('~{merged}') as inf:
    with open("clusterswapped.tsv","w+") as outf:
        # clusterswapped is the same as the metadata input
        # except with the cluster ID field added, and "region" field added
        # to account for blank values. 
        i = 0
        for entry in inf:
            spent = entry.strip().split("\t")
            if i == 0:
                spent.append("cluster")
                spent.append("region")
                i += 1
                print("\t".join(spent),file=outf)
                continue
            #adds cluster id
            # Remember, the set named sd contains samples extracted from column 16 of
            # hardcoded_clusters.tsv, so this test here is asking for this
            # row in metadata_merged.tsv is there a matching sample/strain name in the sd set?
            if spent[0] in sd:
                # if you find a match then add what you found as a value on to the end
                # of this row
                spent.append(sd[spent[0]])
            else:
                # otherwise, add the 'N/A' for not applicable to the end of this 
                # list/array/row found 10,625 of these in JUL-04 fullRefreshRun
                # strains (samples) in metadata_merged.tsv that were not in hardcoded_clusters.tsv
                spent.append("N/A")
            #adds region name
            if spent[0] in rd:
                spent.append(rd[spent[0]].replace("_", " "))
            else:
                # Every specimen had an identifiable region
                # So that means there should not be any strain or samples in clusterswapped.tsv
                # that are not also in sample_regions.tsv.  Is that true?
                spent.append("None")
            i += 1
            print("\t".join(spent),file=outf)
CODE
     >>>
     output {
         File cluster_swap = "clusterswapped.tsv"
     }
     runtime {
        docker: "pathogengenomics/usher:latest" 
        cpu: 32
        memory: "128 GB"
        maxRetries: 2
     }   
}


task prepareTaxonium_states {
    input {
        File clusters_us
        File regions_us
        File merged_us
     }
     command <<<
python3 <<CODE
sd = {}
with open('~{clusters_us}') as inf:
    for entry in inf:
        spent = entry.strip().split("\t")
        if spent[0] == 'cluster_id':
            continue
        for s in spent[-1].split(","):
            sd[s] = spent[0] # sd[sample name] = cluster id
rd = {} 
with open('~{regions_us}') as inf:
    for entry in inf:
        spent = entry.strip().split("\t")
        rd[spent[0]] = spent[1] # rd[sample name] = region
with open('~{merged_us}') as inf:
    with open("clusterswapped_us.tsv","w+") as outf:
        # clusterswapped is the same as the metadata input
        # except with the cluster ID field added, and "region" field added
        # to account for blank values. 
        i = 0
        for entry in inf:
            spent = entry.strip().split("\t")
            if i == 0:
                spent.append("cluster")
                spent.append("region")
                i += 1
                print("\t".join(spent),file=outf)
                continue
            #adds cluster id
            if spent[0] in sd:
                spent.append(sd[spent[0]])
            else:
                spent.append("N/A")
            #adds region name
            if spent[0] in rd:
                spent.append(rd[spent[0]].replace("_", " "))
            else:
                spent.append("None")
            i += 1
            print("\t".join(spent),file=outf)
CODE
     >>>
     output {
         File cluster_swap_us = "clusterswapped_us.tsv"
     }
     runtime {
        docker: "pathogengenomics/usher:latest" 
        cpu: 32
        memory: "128 GB"
        maxRetries: 2
     }   
}


task prepareJSONL {
    input {
        File protobuf
        File cluster_swap
        File genbank
        File overlay
    }
    command <<<

      pip install taxoniumtools

      usher_to_taxonium -i ~{protobuf} -o cview.jsonl.gz -g ~{genbank} -m ~{cluster_swap} -c cluster,genbank_accession,gisaid_accession,country,county,region,date,name,pango_lineage,nextstrain_clade,specimen_id,specimen_accession_number -t "California Big Tree" --overlay_html ~{overlay}

python3 <<CODE

with open("updating.json","w") as outf:
    print('{"status":"updating"}',file=outf)

with open("okay.json","w") as outf:
    print('{"status":"ok"}',file=outf)

CODE
    >>>
    output {
        File cview_jsonl = "cview.jsonl.gz"
        File updating = "updating.json"
        File okay = "okay.json"
    }
    runtime {
        docker: "pathogengenomics/usher:latest" 
        cpu: 32
        memory: "128 GB"
        maxRetries: 2
    }   
}

task prepareJSONL_states {
    input {
        File protobuf
        File cluster_swap_us
        File genbank
        File overlay
    }
    command <<<
        pip install taxoniumtools

        usher_to_taxonium -i ~{protobuf} -o cview_us.jsonl.gz -g ~{genbank} -m ~{cluster_swap_us} -c cluster,genbank_accession,gisaid_accession,country,county,region,date,name,pango_lineage,nextstrain_clade,specimen_id,specimen_accession_number -t "California Big Tree" --overlay_html ~{overlay}
    >>>
    output {
     File cview_us_jsonl = "cview_us.jsonl.gz"
    }
    runtime {
        docker: "pathogengenomics/usher:latest" 
        cpu: 32
        memory: "128 GB"
        maxRetries: 2
    }   
}

task gcs_copy {
  meta { description: "Copied from https://github.com/broadinstitute/viral-pipelines/blob/master/pipes/WDL/tasks/tasks_terra.wdl#L3-L30" }
  input {
    Array[File] infiles
    # File subtree_urls = write_lines(infiles)
    String gcs_uri_prefix
    File html
    File paui2url
    File name 
    File taxonium 
    File subtree 
    File js_regions
    File js_regions_us
    File cluster_data
    File cluster_data_us
    File sample_data
    File sample_data_us
    File pids
    File pids_us
    File cview_jsonl
    File cview_us_jsonl
    File download_clusters
    File download_clusters_us
    File updating
    File public_meta 
    File new_tree
    File translation_table
    File sample_meta 
    File okay
    File tree_jsonl
  }
  parameter_meta {
    infiles: {
      description: "Input files",
      localization_optional: true,
      stream: true
    }
  }
  command <<<
    set -e

    gsutil cp ~{updating} "gs://ucsc-gi-cdph-bigtree/display_tables/status.json"
  
    gsutil cp ~{public_meta} "gs://ucsc-gi-cdph-bigtree/"
    gsutil cp ~{new_tree} "gs://ucsc-gi-cdph-bigtree/"
    gsutil cp ~{translation_table} "gs://ucsc-gi-cdph-bigtree/" 
    gsutil cp ~{sample_meta} "gs://ucsc-gi-cdph-bigtree/"
  
    gsutil cp ~{html} "gs://ucsc-gi-cdph-bigtree/"
    gsutil cp ~{paui2url} "gs://ucsc-gi-cdph-bigtree/"
    gsutil cp ~{name} "gs://ucsc-gi-cdph-bigtree/"
    gsutil cp ~{taxonium} "gs://ucsc-gi-cdph-bigtree/"
    gsutil cp ~{tree_jsonl} "gs://ucsc-gi-cdph-bigtree/"
    gsutil cp ~{subtree} "gs://ucsc-gi-cdph-bigtree/"
    gsutil cp ~{js_regions} "gs://ucsc-gi-cdph-bigtree/display_tables/"
    gsutil cp ~{js_regions} "gs://~{gcs_uri_prefix}/display_tables/"
    gsutil cp ~{js_regions_us} "gs://ucsc-gi-cdph-bigtree/display_tables/"
    gsutil cp ~{js_regions_us} "gs://~{gcs_uri_prefix}/display_tables/"
    gsutil cp ~{cview_jsonl} "gs://ucsc-gi-cdph-bigtree/display_tables/"
    gsutil cp ~{cview_jsonl} "gs://~{gcs_uri_prefix}/display_tables/"
    gsutil cp ~{cview_us_jsonl} "gs://ucsc-gi-cdph-bigtree/display_tables/"
    gsutil cp ~{cview_us_jsonl} "gs://~{gcs_uri_prefix}/display_tables/"
    gsutil cp ~{cluster_data} "gs://~{gcs_uri_prefix}/display_tables"
    gsutil cp ~{cluster_data} "gs://ucsc-gi-cdph-bigtree/display_tables"
    gsutil cp ~{cluster_data_us} "gs://~{gcs_uri_prefix}/display_tables"
    gsutil cp ~{cluster_data_us} "gs://ucsc-gi-cdph-bigtree/display_tables"
    gsutil cp ~{sample_data} "gs://~{gcs_uri_prefix}/display_tables"
    gsutil cp ~{sample_data} "gs://ucsc-gi-cdph-bigtree/display_tables"
    gsutil cp ~{sample_data_us} "gs://~{gcs_uri_prefix}/display_tables"
    gsutil cp ~{sample_data_us} "gs://ucsc-gi-cdph-bigtree/display_tables"
    gsutil cp ~{pids} "gs://~{gcs_uri_prefix}/display_tables"
    gsutil cp ~{pids} "gs://ucsc-gi-cdph-bigtree/display_tables"
    gsutil cp ~{pids_us} "gs://~{gcs_uri_prefix}/display_tables"
    gsutil cp ~{pids_us} "gs://ucsc-gi-cdph-bigtree/display_tables"

    gsutil cp ~{download_clusters} "gs://ucsc-gi-cdph-bigtree/display_tables/"
    gsutil cp ~{download_clusters} "gs://~{gcs_uri_prefix}/display_tables/"
    gsutil cp ~{download_clusters_us} "gs://ucsc-gi-cdph-bigtree/display_tables/"
    gsutil cp ~{download_clusters_us} "gs://~{gcs_uri_prefix}/display_tables/"

    for f in ~{sep=' ' infiles};
    do
      mkdir working
      cd working
      gsutil cp "${f}" .
      tar xzvf *.tar.gz
      cat *.txt | gsutil -m cp -I gs://~{gcs_uri_prefix}
      cd ../
      rm -R working/
    done 

    gsutil cp ~{okay} "gs://ucsc-gi-cdph-bigtree/display_tables/status.json"
   >>>
  output {
    File logs = stdout()
  }
  runtime {
    docker: "quay.io/broadinstitute/viral-baseimage:0.1.20"
    memory: "16 GB"
    cpu: 8
    disks: "local-disk 375 SSD"
  }
}

